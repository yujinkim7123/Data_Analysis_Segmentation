# agents/cx_analyst.py

import json
import numpy as np
import pandas as pd
import networkx as nx

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import MiniBatchKMeans
from sklearn.decomposition import LatentDirichletAllocation
from .utils import get_sentiment_analyzer
from .utils import get_openai_client
from scipy.sparse import csr_matrix # í¬ì†Œ í–‰ë ¬ ë³€í™˜ ì‹œ í•„ìš”
from collections import defaultdict 

# --- ë‚´ë¶€ í—¬í¼(ë³´ì¡°) í•¨ìˆ˜ë“¤ ---
def _get_sentiment_score(text: str) -> float:
    """í…ìŠ¤íŠ¸ì˜ ê°ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (ë™ê¸° ë²„ì „)"""
    analyzer = get_sentiment_analyzer() # ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ await ì—†ìŒ
    if analyzer is None:
        print("âŒ ê°ì„± ë¶„ì„ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê°ì„± ì ìˆ˜ ê³„ì‚° ë¶ˆê°€.")
        return 0.0 # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½ ì ìˆ˜ ë°˜í™˜
    
    try:
        # Hugging Face pipeline ê²°ê³¼ëŠ” ë¦¬ìŠ¤íŠ¸ [{label: 'LABEL_0', score: 0.99}] í˜•íƒœ
        # bert-nsmc ëª¨ë¸ì€ LABEL_0 (ë¶€ì •)ê³¼ LABEL_1 (ê¸ì •)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        result = analyzer(text[:512])[0] # ëª¨ë¸ ì…ë ¥ ê¸¸ì´ ì œí•œ (ì˜ˆ: 512)
        
        # LABEL_1 (ê¸ì •)ì€ ê¸ì • ì ìˆ˜, LABEL_0 (ë¶€ì •)ì€ ë¶€ì • ì ìˆ˜ë¡œ ë§¤í•‘
        if result['label'] == 'LABEL_1': # ê¸ì •
            return float(result['score'])
        elif result['label'] == 'LABEL_0': # ë¶€ì •
            return -float(result['score'])
        else: # ì¤‘ë¦½ ë˜ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” ê²½ìš° (ì—†ì„ ê°€ëŠ¥ì„± ë†’ìŒ)
            return 0.0
    except Exception as e:
        print(f"ê°ì„± ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 0.0 # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë¦½ ì ìˆ˜ ë°˜í™˜

def _get_top_keywords(feature_names, topic_components, n_top_words):
    """
    LDA í† í”½ ëª¨ë¸ì˜ ì»´í¬ë„ŒíŠ¸(ë‹¨ì–´-í† í”½ ë¶„í¬)ì—ì„œ ê° í† í”½ë³„ ìƒìœ„ Nê°œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    Args:
        feature_names (list): TF-IDF ë²¡í„°ë¼ì´ì €ì˜ í”¼ì²˜(ë‹¨ì–´) ì´ë¦„ ëª©ë¡.
        topic_components (np.array): LDA ëª¨ë¸ì˜ .components_ ì†ì„± (í† í”½-ë‹¨ì–´ ë¶„í¬ í–‰ë ¬).
        n_top_words (int): ê° í† í”½ì—ì„œ ì¶”ì¶œí•  ìƒìœ„ í‚¤ì›Œë“œ ê°œìˆ˜.
    Returns:
        list of lists: ê° í† í”½ë³„ ìƒìœ„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸.
    """
    top_keywords = []
    for topic_idx, topic in enumerate(topic_components):
        # topicì€ í•´ë‹¹ í† í”½ì˜ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ í¬í•¨í•˜ëŠ” NumPy ë°°ì—´ì…ë‹ˆë‹¤.
        # argsort()[-n_top_words-1:-1:-1]ëŠ” ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ìƒìœ„ n_top_words ê°œì˜ ì¸ë±ìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.
        top_words_indices = topic.argsort()[:-n_top_words - 1:-1]
        
        # í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ë‹¨ì–´ë“¤ì„ feature_namesì—ì„œ ê°€ì ¸ì™€ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤.
        keywords_for_topic = [feature_names[i] for i in top_words_indices]
        top_keywords.append(keywords_for_topic)
    return top_keywords

# --- Master Agentê°€ í˜¸ì¶œí•  Tool í•¨ìˆ˜ë“¤ ---


def run_ward_clustering(workspace, num_clusters=5):
    """
    ê³ ê°ì˜ ëª©ì†Œë¦¬(VOC) ë°ì´í„°ë¥¼ ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ ì£¼ìš” ì£¼ì œ ê·¸ë£¹ì„ ë°œê²¬í•˜ê³ ,
    ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    print(f"[CX Analysis] Running Ward Clustering with {num_clusters} clusters (ë™ê¸° ëª¨ë“œ)")

    retrieved_data = workspace["artifacts"].get("retrieved_data")
    if not retrieved_data:
        return {"error": "ë°ì´í„° í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ ê²€ìƒ‰ëœ ë°ì´í„°ê°€ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„° ê²€ìƒ‰ì„ í•´ì£¼ì„¸ìš”."}

    documents = [d.get('original_text', '') for d in retrieved_data.get('web_results', []) if d.get('original_text')]
    
    if not documents:
        return {"error": "í´ëŸ¬ìŠ¤í„°ë§í•  ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."}

    try:
        # 2. í…ìŠ¤íŠ¸ ë²¡í„°í™” (TF-IDF)
        # max_featuresëŠ” ë‹¨ì–´ì˜ ìˆ˜ë¥¼ ì œí•œí•˜ê³ , min_df/max_dfëŠ” ë„ˆë¬´ ì ê±°ë‚˜ ë„ˆë¬´ ë§ì€ ë¬¸ì„œì— ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ë¥¼ ì œì™¸í•©ë‹ˆë‹¤.
        # stop_words='english'ëŠ” ì˜ì–´ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ì§€ë§Œ, í•œêµ­ì–´ ë°ì´í„°ì—ëŠ” ì í•©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ Noneì„ ìœ ì§€í•©ë‹ˆë‹¤.
        # í•œêµ­ì–´ ë¶ˆìš©ì–´ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë‹¤ë©´, ì§ì ‘ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
        vectorizer = TfidfVectorizer(max_features=2000, min_df=0.01, max_df=0.9) # min_df, max_df ë²”ìœ„ ì¡°ì •
        X = vectorizer.fit_transform(documents)

        if X.shape[1] == 0: # ë¬¸ì„œ-ë‹¨ì–´ í–‰ë ¬ì— ìœ íš¨í•œ í”¼ì²˜(ë‹¨ì–´)ê°€ ì—†ëŠ” ê²½ìš°
            return {"error": "TF-IDF ë²¡í„°í™” í›„ ìœ íš¨í•œ ë‹¨ì–´ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•˜ê±°ë‚˜ TfidfVectorizer ì„¤ì •ì„ ì¡°ì •í•˜ì„¸ìš”."}

        # 3. K-Means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        # num_clustersê°€ ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ë³´ë‹¤ í¬ë©´ ì˜¤ë¥˜ê°€ ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì œí•œí•©ë‹ˆë‹¤.
        if num_clusters > X.shape[0]:
            num_clusters = X.shape[0] # ë¬¸ì„œ ìˆ˜ë³´ë‹¤ í´ëŸ¬ìŠ¤í„° ìˆ˜ê°€ ë§ì„ ìˆ˜ ì—†ìŒ
            print(f"âš ï¸ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ê°€ ë¬¸ì„œ ìˆ˜ë³´ë‹¤ ë§ì•„ {num_clusters}ê°œë¡œ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ê°€ 1ì´ë©´ í´ëŸ¬ìŠ¤í„°ë§ì˜ ì˜ë¯¸ê°€ ì—†ê±°ë‚˜ K-Means ì˜¤ë¥˜ ê°€ëŠ¥
        if num_clusters < 2:
            return {"error": "í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ëŠ” ìµœì†Œ 2ê°œ ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤."}

        kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=42, n_init=10)
        kmeans.fit(X)
        cluster_labels = kmeans.labels_.tolist()

        # 4. ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ í‚¤ì›Œë“œ ì¶”ì¶œ
        feature_names = np.array(vectorizer.get_feature_names_out())
        cluster_centers = kmeans.cluster_centers_

        cluster_summaries = {}
        for i in range(num_clusters):
            # í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ë¬¸ì„œ ì¸ë±ìŠ¤ ì°¾ê¸°
            cluster_docs_indices = np.where(kmeans.labels_ == i)[0]
            num_docs_in_cluster = len(cluster_docs_indices)

            # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if num_docs_in_cluster == 0:
                cluster_summaries[str(i)] = {"keywords": [], "description": f"{i}ë²ˆ ê·¸ë£¹ (0ê°œ ë¬¸ì„œ)ì—ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."}
                continue

            # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ TF-IDF ë²¡í„° í•©ê³„ ê³„ì‚° (ê°€ì¥ ì¤‘ìš”)
            # í´ëŸ¬ìŠ¤í„° ë‚´ ë¬¸ì„œë“¤ì˜ ë‹¨ì–´ ì¤‘ìš”ë„ë¥¼ í•©ì‚°í•˜ì—¬ í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ì „ë°˜ì ì¸ ë‹¨ì–´ ë¶„í¬ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
            cluster_tfidf_sum = X[cluster_docs_indices].sum(axis=0)

            # Sumëœ ë²¡í„°ì—ì„œ ê°€ì¥ ë†’ì€ TF-IDF ê°’ì„ ê°€ì§„ ë‹¨ì–´ ì¸ë±ìŠ¤ ì°¾ê¸°
            # .flatten()ìœ¼ë¡œ 1D ë°°ì—´ë¡œ ë§Œë“  í›„ argsort ì‚¬ìš©
            top_feature_indices = cluster_tfidf_sum.A.flatten().argsort()[-10:][::-1] # .AëŠ” í¬ì†Œ í–‰ë ¬ì„ ë°€ì§‘ í–‰ë ¬ë¡œ ë³€í™˜

            top_keywords = feature_names[top_feature_indices].tolist()

            cluster_summaries[str(i)] = {
                "keywords": top_keywords,
                "description": f"{i}ë²ˆ ê·¸ë£¹ ({num_docs_in_cluster}ê°œ ë¬¸ì„œ)ì€ ì£¼ë¡œ '{', '.join(top_keywords[:5])}...' ë“±ì˜ í‚¤ì›Œë“œë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
            }
        
        # 5. ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì„ì‹œ ë°ì´í„° ì €ì¥ (LDA, SNAë¥¼ ìœ„í•´)
        # X (TF-IDF í–‰ë ¬)ëŠ” í¬ì†Œ í–‰ë ¬ì´ë¯€ë¡œ, ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        # toarray().tolist()ëŠ” ë§¤ìš° í° ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜ (ëŒ€ìš©ëŸ‰ ë°ì´í„°ì˜ ê²½ìš° ë‹¤ë¥¸ ë°©ì‹ ê³ ë ¤)
        tfidf_matrix_list = X.toarray().tolist()

        # feature_namesëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        feature_names_list = feature_names.tolist()

        cluster_docs_map = defaultdict(list)
        for doc_idx, label in enumerate(cluster_labels):
            cluster_docs_map[label].append(doc_idx)

        workspace["artifacts"]["_cx_temp_data"] = {
            "cluster_labels": cluster_labels,
            "tfidf_matrix": tfidf_matrix_list, # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì €ì¥
            "feature_names": feature_names_list, # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì €ì¥
             "documents": documents, # ğŸ‘ˆ ì›ë³¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
             "cluster_docs_map": dict(cluster_docs_map) # ğŸ‘ˆ defaultdictë¥¼ ì¼ë°˜ dictë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€
        }

        # 6. ë¶„ì„ ê²°ê³¼ ë°˜í™˜
        return {
            "cx_ward_clustering_results": {
                "num_clusters": num_clusters,
                "cluster_labels": cluster_labels,
                "cluster_summaries": cluster_summaries,
            },
            "analysis_results": "Ward clustering analysis complete. ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ë³´ì„¸ìš”. íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ ë” ê¹Šì€ ë¶„ì„(ì˜ë¯¸ ì—°ê²°ë§ ë¶„ì„)ì„ ì›í•˜ì‹œë©´ í´ëŸ¬ìŠ¤í„° IDì™€ í•¨ê»˜ ìš”ì²­í•´ì£¼ì„¸ìš”."
        }

    except Exception as e:
        print(f"âŒ ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return {"error": f"ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}


def run_semantic_network_analysis(workspace: dict, cluster_id: int):
    """PDF 2ë‹¨ê³„: íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ SNAë¥¼ ìˆ˜í–‰í•˜ì—¬ í•µì‹¬ ë…¸ë“œë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    print(f"âœ… [CX Agent] Step 2: Running SNA for Cluster ID: {cluster_id} (ë™ê¸° ëª¨ë“œ)")
    temp_data = workspace.get("artifacts", {}).get("_cx_temp_data", {})
    
    if not temp_data.get("cluster_labels"): return {"error": "êµ°ì§‘í™”ë¥¼ ë¨¼ì € ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤."}
    if not temp_data.get("tfidf_matrix"): return {"error": "TF-IDF í–‰ë ¬ì´ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤."}
    if not temp_data.get("feature_names"): return {"error": "í”¼ì²˜ ì´ë¦„ì´ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤."}

    try: # ì „ì²´ ë¡œì§ì„ try-exceptë¡œ ê°ì‹¸ì„œ ì•ˆì •ì„± í™•ë³´
        # 1. íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë“¤ì˜ TF-IDF í–‰ë ¬ ì¶”ì¶œ
        docs_indices = [i for i, label in enumerate(temp_data["cluster_labels"]) if label == cluster_id]
        if not docs_indices: return {"error": f"IDê°€ {cluster_id}ì¸ í´ëŸ¬ìŠ¤í„°ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # temp_data["tfidf_matrix"]ëŠ” ì´ì œ ì¼ë°˜ Python ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ,
        # ë‹¤ì‹œ í¬ì†Œ í–‰ë ¬ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
        cluster_matrix = csr_matrix(np.array(temp_data["tfidf_matrix"])[docs_indices])

        # 2. ë‹¨ì–´ ë™ì‹œ ì¶œí˜„ í–‰ë ¬ ê³„ì‚°
        co_occurrence_matrix = (cluster_matrix.T * cluster_matrix)
        co_occurrence_matrix.setdiag(0) # ìê¸° ìì‹ ê³¼ì˜ ë™ì‹œ ì¶œí˜„ì€ 0ìœ¼ë¡œ ì„¤ì •

        # 3. ë„¤íŠ¸ì›Œí¬X ê·¸ë˜í”„ ìƒì„± ë° ì¤‘ì‹¬ì„± ê³„ì‚°
        G = nx.from_scipy_sparse_array(co_occurrence_matrix)
        # âš ï¸ ë””ë²„ê¹… ëª©ì ì´ ì•„ë‹ˆë¼ë©´, G ê°ì²´ë‚˜ co_occurrence_matrixë¥¼ ì§ì ‘ print()í•˜ì§€ ë§ˆì„¸ìš”.
        # print(f"DEBUG: Graph G: {G}") # ì´ ì¤„ì²˜ëŸ¼ ê°ì²´ ìì²´ë¥¼ ì¶œë ¥í•˜ëŠ” ê²ƒì€ í”¼í•´ì•¼ í•©ë‹ˆë‹¤.

        centrality = nx.degree_centrality(G)
        sorted_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)
        
        # 4. í•µì‹¬ ë…¸ë“œ (í•µì‹¬ í‚¤ì›Œë“œ) ì¶”ì¶œ
        feature_names = temp_data["feature_names"]

        micro_segments = []
        for node_idx, score in sorted_nodes[:10]:
            if node_idx < len(feature_names):
                micro_segments.append({
                    "core_keyword": feature_names[node_idx],
                    "centrality_score": round(score, 4)
                })
            else:
                print(f"âš ï¸ ê²½ê³ : SNA ë…¸ë“œ ì¸ë±ìŠ¤ {node_idx}ê°€ feature_names ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤.")

        # 5. ê²°ê³¼ ë°˜í™˜ (ì§ë ¬í™” ê°€ëŠ¥í•œ ë°ì´í„°ë§Œ í¬í•¨)
        return {
            "cx_sna_results": {
                "cluster_id": cluster_id,
                "micro_segments": micro_segments,
                "analysis_description": f"{cluster_id}ë²ˆ í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ ê°€ì¥ í•µì‹¬ì ì¸ í‚¤ì›Œë“œë“¤ì„ ì°¾ì•„ ì˜ë¯¸ ì—°ê²°ë§ ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤."
            },
            "analysis_results": "Semantic network analysis complete." # ì´ ë¶€ë¶„ì€ Pydanticì´ ì§ë ¬í™” ê°€ëŠ¥í•´ì•¼ í•¨
        }

    except Exception as e:
        print(f"âŒ SNA ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return {"error": f"SNA ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}


def run_topic_modeling_lda(workspace: dict, cluster_id: int, num_topics: int = 3):
    """
    PDF 3ë‹¨ê³„: íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ LDAë¥¼ ìˆ˜í–‰í•˜ì—¬ êµ¬ì²´ì ì¸ 'ê³ ê° ì•¡ì…˜'ì„ ì‹ë³„í•©ë‹ˆë‹¤.
    """
    print(f"âœ… [CX Agent] Step 3: Running LDA for Cluster ID: {cluster_id} (ë™ê¸° ëª¨ë“œ)")
    artifacts = workspace.get("artifacts", {}) # artifactsë¥¼ ë¨¼ì € ê°€ì ¸ì˜µë‹ˆë‹¤.
    temp_data = artifacts.get("_cx_temp_data", {}) # _cx_temp_dataëŠ” artifacts ì•ˆì— ìˆìŠµë‹ˆë‹¤.
    
    # 1. í•„ìˆ˜ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬
    if not temp_data.get("cluster_labels"):
        return {"error": "í† í”½ ëª¨ë¸ë§ì„ ìœ„í•´ì„œëŠ” êµ°ì§‘í™”ë¥¼ ë¨¼ì € ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤."}
    if not temp_data.get("tfidf_matrix"):
        return {"error": "í† í”½ ëª¨ë¸ë§ì„ ìœ„í•œ TF-IDF í–‰ë ¬ì´ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤."}
    if not temp_data.get("feature_names"):
        return {"error": "í† í”½ ëª¨ë¸ë§ì„ ìœ„í•œ í”¼ì²˜ ì´ë¦„ì´ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤."}

    try:
        # 2. íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë“¤ì˜ TF-IDF í–‰ë ¬ ì¶”ì¶œ
        docs_indices = [i for i, label in enumerate(temp_data["cluster_labels"]) if label == cluster_id]
        if not docs_indices:
            return {"error": f"IDê°€ {cluster_id}ì¸ í´ëŸ¬ìŠ¤í„°ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # _cx_temp_dataì— ì €ì¥ëœ tfidf_matrixëŠ” list of lists (Python ê°ì²´)ì´ë¯€ë¡œ
        # ë‹¤ì‹œ í¬ì†Œ í–‰ë ¬ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
        doc_term_matrix = csr_matrix(np.array(temp_data["tfidf_matrix"])[docs_indices])
        
        # 3. í”¼ì²˜ ì´ë¦„(ë‹¨ì–´ ëª©ë¡) ê°€ì ¸ì˜¤ê¸°
        feature_names = temp_data["feature_names"] # TfidfVectorizer ê°ì²´ ëŒ€ì‹  ì €ì¥ëœ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©

        # 4. LDA ëª¨ë¸ í•™ìŠµ
        lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
        lda.fit(doc_term_matrix) # ë¬¸ì„œ-ë‹¨ì–´ í–‰ë ¬ë¡œ LDA í•™ìŠµ

        # ê° ë¬¸ì„œì˜ í† í”½ ë¶„í¬ë¥¼ ê³„ì‚° (calculate_opportunity_scoresì—ì„œ ì‚¬ìš©ë  ë°ì´í„°)
        doc_topic_dist_for_cluster = lda.transform(doc_term_matrix) # ğŸ‘ˆ í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë“¤ì˜ í† í”½ ë¶„í¬

        # 5. í† í”½ë³„ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        topics_list = []
        top_keywords_per_topic = _get_top_keywords(feature_names, lda.components_, 7) # ìƒìœ„ 7ê°œ í‚¤ì›Œë“œ

        for i, keywords in enumerate(top_keywords_per_topic):
            topics_list.append({
                "topic_id": f"{cluster_id}-{i}", # í´ëŸ¬ìŠ¤í„° IDì™€ í† í”½ ì¸ë±ìŠ¤ë¥¼ ì¡°í•©
                "action_keywords": keywords,
                "description": f"ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(keywords[:3])}..." # ê°„ë‹¨í•œ ì„¤ëª… ì¶”ê°€
            })

    
        workspace["artifacts"]["_cx_temp_data"]["doc_topic_distribution"] = doc_topic_dist_for_cluster.tolist()
        workspace["artifacts"]["_cx_temp_data"]["lda_components_for_cluster"] = lda.components_.tolist() # í•„ìš”ì‹œ ì €ì¥

        # ğŸš¨ğŸš¨ğŸš¨ cx_lda_results ë¦¬ìŠ¤íŠ¸ì— í† í”½ ê²°ê³¼ ëˆ„ì  ì €ì¥ (main.pyì—ì„œ []ë¡œ ì´ˆê¸°í™”ë¨)
        # ì´ë¯¸ ì´ ë¡œì§ì€ ì˜ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
        current_lda_results = artifacts.get("cx_lda_results", [])
        current_lda_results.extend(topics_list)
        artifacts["cx_lda_results"] = current_lda_results # artifacts ë”•ì…”ë„ˆë¦¬ì— ì—…ë°ì´íŠ¸

        # 7. LLMì—ê²Œ ë°˜í™˜í•  ë°ì´í„° (ê°„ê²°í•˜ê²Œ)
        return {
            "success": True,
            "message": f"í´ëŸ¬ìŠ¤í„° {cluster_id}ì— ëŒ€í•´ {num_topics}ê°œì˜ í† í”½ì„ ì„±ê³µì ìœ¼ë¡œ ì‹ë³„í–ˆìŠµë‹ˆë‹¤.",
            "newly_identified_topics_preview": [
                {"topic_id": t["topic_id"], "action_keywords": t["action_keywords"]}
                for t in topics_list
            ]
        }

    except Exception as e:
        print(f"âŒ í† í”½ ëª¨ë¸ë§(LDA) ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc() # ìƒì„¸ ì—ëŸ¬ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥
        return {"error": f"í† í”½ ëª¨ë¸ë§(LDA) ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}

def create_customer_action_map(workspace: dict, topic_id: str):
    """
    [ì™„ì„±ë³¸] PDF 4ë‹¨ê³„: 'ë¶„ì„ëœ ê²°ê³¼'ë¥¼ ë°”íƒ•ìœ¼ë¡œ CAM(Pain Point ë“±)ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print(f"âœ… [CX Agent] Step 4: Creating CAM for Topic ID: {topic_id}...")
    client = get_openai_client()
    
    # --- 1. workspaceì—ì„œ ì´ í† í”½ì— ëŒ€í•œ ëª¨ë“  ë¶„ì„ 'ê²°ê³¼'ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ---
    artifacts = workspace.get("artifacts", {})
    lda_results = artifacts.get("cx_lda_results", [])
    opportunity_scores = artifacts.get("cx_opportunity_scores", [])
    
    # í•´ë‹¹ topic_idì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    topic_lda_data = next((item for item in lda_results if item.get("topic_id") == topic_id), None)
    topic_score_data = next((item for item in opportunity_scores if item.get("topic_id") == topic_id), None)

    if not topic_lda_data or not topic_score_data:
        return {"error": f"IDê°€ {topic_id}ì¸ í† í”½ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. LDAì™€ ê¸°íšŒ ì ìˆ˜ ê³„ì‚°ì„ ë¨¼ì € ìˆ˜í–‰í•´ì£¼ì„¸ìš”."}
    
    action_keywords = topic_lda_data.get('action_keywords', [])
    first_keyword = action_keywords[0] if action_keywords else topic_id # ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ topic_id ì‚¬ìš©

    # --- 2. LLMì—ê²Œ ì „ë‹¬í•  'ë¶„ì„ ìš”ì•½ ì •ë³´'ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ---
    prompt = f"""
    ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ í•´ì„í•˜ì—¬ ê³ ê° ì•¡ì…˜ë§µ(CAM)ì„ ì™„ì„±í•˜ëŠ” ìµœê³ ì˜ CX ì „ëµê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ëŠ” íŠ¹ì • ê³ ê° í–‰ë™(Action)ì— ëŒ€í•œ ì •ëŸ‰ì /ì •ì„±ì  ë¶„ì„ ìš”ì•½ ê²°ê³¼ì…ë‹ˆë‹¤.

    [ë¶„ì„ ë°ì´í„° ìš”ì•½]
    - í–‰ë™(Action) ID: {topic_id}
    - í–‰ë™ì˜ í•µì‹¬ í‚¤ì›Œë“œ: "{', '.join(topic_lda_data.get('action_keywords', []))}"
    - ì´ í–‰ë™ì— ëŒ€í•œ ê³ ê° ë§Œì¡±ë„ ì ìˆ˜: {topic_score_data.get('satisfaction')} (-1.0: ë§¤ìš° ë¶€ì •, 1.0: ë§¤ìš° ê¸ì •)
    - ì´ í–‰ë™ì˜ ì¤‘ìš”ë„(ì–¸ê¸‰ëŸ‰): {topic_score_data.get('importance')}

    ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì´ í–‰ë™ì„ í•˜ëŠ” ê³ ê°ë“¤ì˜ 'Goal(ê¶ê·¹ì  ëª©í‘œ)'ê³¼ 'Pain Point(í•µì‹¬ ë¶ˆí¸í•¨)'ë¥¼ ê°ê° 2~3ê°€ì§€ì”© ê¹Šì´ ìˆê²Œ ì¶”ë¡ í•´ì£¼ì„¸ìš”.
    PDFì˜ CAM í”„ë ˆì„ì›Œí¬ë¥¼ ì°¸ê³ í•˜ì—¬, ì´ í–‰ë™ì´ ì£¼ë¡œ ë°œìƒí•˜ëŠ” 'Context(ìƒí™©)'ì™€ ê´€ë ¨ëœ 'Touchpoint/Artifact(ì‚¬ë¬¼/ì„œë¹„ìŠ¤)'ë„ í•¨ê»˜ ì¶”ë¡ í•˜ì—¬ ì œì‹œí•´ì£¼ì„¸ìš”.

    ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì•„ë˜ì˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
    {{
      "action_name": "{first_keyword}",
      "goals": ["ì¶”ë¡ ëœ ëª©í‘œ 1", "ì¶”ë¡ ëœ ëª©í‘œ 2"],
      "pain_points": ["ì¶”ë¡ ëœ ë¶ˆí¸í•¨ 1", "ì¶”ë¡ ëœ ë¶ˆí¸í•¨ 2"],
      "context": ["ì¶”ë¡ ëœ ìƒí™© 1", "ì¶”ë¡ ëœ ìƒí™© 2"],
      "touchpoint_artifact": ["ê´€ë ¨ëœ ì‚¬ë¬¼ 1", "ê´€ë ¨ëœ ì‚¬ë¬¼ 2"]
    }}
    """
    try:
        res = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        cam_results = json.loads(res.choices[0].message.content)
        
        existing_cams = workspace.get("artifacts", {}).get("cx_cam_results", [])
        existing_cams.append(cam_results)
        
        return {"cx_cam_results": existing_cams}
    except Exception as e:
        return {"error": f"ê³ ê° ì•¡ì…˜ë§µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"}


def calculate_opportunity_scores(workspace: dict):
    """
    [ì™„ì„±ë³¸] PDF 5ë‹¨ê³„: ëª¨ë“  í† í”½ì˜ ê¸°íšŒ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ìš°ì„ ìˆœìœ„ë¥¼ ì •í•©ë‹ˆë‹¤.
    """
    print("âœ… [CX Agent] Step 5: Calculating Opportunity Scores...")
    client = get_openai_client()
    
    # --- 1. workspaceì—ì„œ ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ë¬¼ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. ---
    artifacts = workspace.get("artifacts", {})
    temp_data = artifacts.get("_cx_temp_data", {})
    lda_results = artifacts.get("cx_lda_results", []) # ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì˜ LDA í† í”½ ë¦¬ìŠ¤íŠ¸ (main.pyì—ì„œ []ë¡œ ì´ˆê¸°í™”ë¨)
    
    # ğŸš¨ğŸš¨ğŸš¨ ì´ ë¶€ë¶„ë“¤ì„ ìˆ˜ì •í•©ë‹ˆë‹¤: lda_model, vectorizer ëŒ€ì‹  ì§ë ¬í™” ê°€ëŠ¥í•œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
    documents = temp_data.get("documents") # ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§ì—ì„œ ì €ì¥ë¨
    doc_topic_distribution = temp_data.get("doc_topic_distribution") # LDA í† í”½ ëª¨ë¸ë§ì—ì„œ ì €ì¥ë¨ (list of lists)
    tfidf_matrix_all = temp_data.get("tfidf_matrix") # ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§ì—ì„œ ì €ì¥ëœ ì „ì²´ TF-IDF í–‰ë ¬ (list of lists)
    feature_names = temp_data.get("feature_names") # ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§ì—ì„œ ì €ì¥ëœ ë‹¨ì–´ ëª©ë¡
    cluster_docs_map = temp_data.get("cluster_docs_map") # í´ëŸ¬ìŠ¤í„°ë³„ ë¬¸ì„œ ì¸ë±ìŠ¤ ë§µ (ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§ì—ì„œ ì €ì¥ë¨)
    
    print(f"ğŸ” [Debug-Opportunity] documents: {'None' if documents is None else 'OK'}, "
          f"doc_topic_distribution: {'None' if doc_topic_distribution is None else 'OK'}, "
          f"tfidf_matrix_all: {'None' if tfidf_matrix_all is None else 'OK'}, "
          f"feature_names: {'None' if feature_names is None else 'OK'}, "
          f"lda_results: {'None' if not lda_results else 'OK'}, " # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë„ Falseì´ë¯€ë¡œ `not` ì‚¬ìš©
          f"cluster_docs_map: {'None' if cluster_docs_map is None else 'OK'}")

    # í•„ìˆ˜ ë°ì´í„° ëˆ„ë½ í™•ì¸ (ì¡°ê±´ë¬¸ ìˆ˜ì •)
    if not all([documents, doc_topic_distribution is not None, tfidf_matrix_all is not None,
                 feature_names is not None, lda_results, cluster_docs_map]):
        return {"error": "ê¸°íšŒ ì ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ ë¶„ì„ ê²°ê³¼(ë¬¸ì„œ, í† í”½ ë¶„í¬, TF-IDF í–‰ë ¬, LDA í† í”½, í´ëŸ¬ìŠ¤í„° ë§µ, í”¼ì²˜ ì´ë¦„)ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì´ì „ ë¶„ì„ ë‹¨ê³„ë¥¼ ëª¨ë‘ ìˆ˜í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."}
    
    # ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
    doc_topic_distribution_np = np.array(doc_topic_distribution) # list of lists -> numpy array

    scores = []
    
    # --- 2. ê° í† í”½(Action)ë³„ë¡œ ìˆœíšŒí•˜ë©° ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ---
    for topic_item in lda_results: # lda_resultsëŠ” ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•¨
        topic_id_str = topic_item.get("topic_id") # ì˜ˆ: "2-1" (í´ëŸ¬ìŠ¤í„°ID-í† í”½ì¸ë±ìŠ¤)
        action_keywords = topic_item.get("action_keywords") # LDAì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œ
        
        # topic_id_strì´ ìœ íš¨í•œ í˜•íƒœì¸ì§€ í™•ì¸ (ì˜ˆ: "C-T" í˜•ì‹)
        if '-' not in topic_id_str:
            print(f"âš ï¸ ê²½ê³ : ìœ íš¨í•˜ì§€ ì•Šì€ topic_id í˜•ì‹: {topic_id_str}. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            continue
        
        try:
            cluster_id = int(topic_id_str.split('-')[0])
            topic_index_in_cluster = int(topic_id_str.split('-')[1])
        except ValueError:
            print(f"âš ï¸ ê²½ê³ : topic_id {topic_id_str}ì—ì„œ í´ëŸ¬ìŠ¤í„° ID ë˜ëŠ” í† í”½ ì¸ë±ìŠ¤ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            continue

        # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ëŠ” ì›ë³¸ ë¬¸ì„œë“¤ì˜ ì¸ë±ìŠ¤
        cluster_docs_indices_in_all_docs = cluster_docs_map.get(cluster_id)
        if cluster_docs_indices_in_all_docs is None or not cluster_docs_indices_in_all_docs:
            print(f"âš ï¸ ê²½ê³ : í´ëŸ¬ìŠ¤í„° {cluster_id}ì— ëŒ€í•œ ë¬¸ì„œ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            continue

        # ğŸš¨ğŸš¨ğŸš¨ ì´ ë¶€ë¶„ë“¤ì„ ìˆ˜ì •í•©ë‹ˆë‹¤: lda_modelê³¼ vectorizer ëŒ€ì‹  ì €ì¥ëœ ë°ì´í„°ë¥¼ í™œìš©
        # doc_topic_distribution_npëŠ” run_topic_modeling_ldaì—ì„œ ê³„ì‚°ëœ í´ëŸ¬ìŠ¤í„° ë¬¸ì„œë“¤ì— ëŒ€í•œ ë¶„í¬ì…ë‹ˆë‹¤.
        # ë”°ë¼ì„œ, ì—¬ê¸°ì„œ ë‹¤ì‹œ transform í•  í•„ìš” ì—†ì´, ë°”ë¡œ í•´ë‹¹ í† í”½ì˜ ë¬¸ì„œë¥¼ ì°¾ìœ¼ë©´ ë©ë‹ˆë‹¤.
        
        TOPIC_ASSIGNMENT_THRESHOLD = 0.05

        
        # 3. ì„ê³„ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ í•„í„°ë§
        docs_belonging_to_this_topic_indices_by_threshold = np.where(doc_topic_distribution_np[:, topic_index_in_cluster] >= TOPIC_ASSIGNMENT_THRESHOLD)[0]

        # 4. (ì„ íƒ ì‚¬í•­) ê°€ì¥ ë†’ì€ í™•ë¥ ë¡œ í• ë‹¹ëœ ë¬¸ì„œë„ í•¨ê»˜ í™•ì¸ (ë””ë²„ê¹…ìš©)
        docs_most_strongly_assigned_to_this_topic_indices = np.where(np.argmax(doc_topic_distribution_np, axis=1) == topic_index_in_cluster)[0]

        print(f"ğŸ” [Debug-TopicDocs] Topic {topic_id_str}: "
              f"Docs by Threshold ({TOPIC_ASSIGNMENT_THRESHOLD}): {len(docs_belonging_to_this_topic_indices_by_threshold)} / {doc_topic_distribution_np.shape[0]}, "
              f"Docs by Argmax: {len(docs_most_strongly_assigned_to_this_topic_indices)} / {doc_topic_distribution_np.shape[0]}")


        # ì‹¤ì œ ì›ë³¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ í† í”½ì˜ ë¬¸ì„œë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # ì„ê³„ê°’ ê¸°ì¤€ìœ¼ë¡œ ì°¾ì€ ë¬¸ì„œë¥¼ ì‚¬ìš©
        actual_original_docs_indices = [cluster_docs_indices_in_all_docs[i] for i in docs_belonging_to_this_topic_indices_by_threshold]
        topic_docs = [documents[i] for i in actual_original_docs_indices]
        
        if not topic_docs:
            # ê²½ê³  ë©”ì‹œì§€ì— Argmax ê¸°ì¤€ ë¬¸ì„œ ìˆ˜ë„ ì¶”ê°€í•˜ì—¬ ë¹„êµ
            print(f"âš ï¸ ê²½ê³ : í† í”½ {topic_id_str}ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. "
                  f"(ì„ê³„ê°’ {TOPIC_ASSIGNMENT_THRESHOLD} ê¸°ì¤€: 0ê°œ, Argmax ê¸°ì¤€: {len(docs_most_strongly_assigned_to_this_topic_indices)}ê°œ) ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            continue

        # 3. ì¤‘ìš”ë„(ì–¸ê¸‰ëŸ‰) ê³„ì‚°: í•´ë‹¹ í† í”½ì— ì†í•˜ëŠ” ì‹¤ì œ ë¬¸ì„œì˜ ìˆ˜
        importance = len(topic_docs)
        
        # 4. ë§Œì¡±ë„(ê°ì„± ì ìˆ˜) ê³„ì‚° - ë™ê¸° ì²˜ë¦¬ (ìƒ˜í”Œë§)
        sample_size = min(len(topic_docs), 20)
        sentiments = [_get_sentiment_score(doc) for doc in topic_docs[:sample_size]]
        satisfaction = np.mean(sentiments) if sentiments else 0.0
        
        # 5. ê¸°íšŒ ì ìˆ˜ ê³„ì‚°
        opportunity_score = importance * max(0, -satisfaction) # ë¶€ì •ì  ê°ì„±ì¼ìˆ˜ë¡ ê¸°íšŒê°€ í¼

        scores.append({
            "topic_id": topic_id_str,
            "action_keywords": action_keywords,
            "importance": importance, # ì–¸ê¸‰ëŸ‰
            "satisfaction": round(satisfaction, 2), # ê°ì„± ì ìˆ˜ (-1.0 ~ 1.0)
            "opportunity_score": round(opportunity_score, 2)
        })
        
    scores.sort(key=lambda x: x['opportunity_score'], reverse=True)

    # 6. ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ê²°ê³¼ ì €ì¥
    workspace["artifacts"]["cx_opportunity_scores"] = scores # ì´ë¯¸ main.pyì—ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ì˜ë¨

    return {"cx_opportunity_scores": scores}
