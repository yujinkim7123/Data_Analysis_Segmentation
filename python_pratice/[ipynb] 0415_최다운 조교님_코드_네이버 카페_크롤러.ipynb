{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import csv\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium.common.exceptions import UnexpectedAlertPresentException\n",
    "import urllib.request\n",
    "import random\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from tqdm import tqdm\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 키워드 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = ['샘플입니당']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로그인 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# 크롬 드라이버 제어\n",
    "#driver = webdriver.Chrome() #현재 컴퓨터 크롬드라이버 위치로 변경\n",
    "#driver = webdriver.Chrome(service= Service(ChromeDriverManager().install()))\n",
    "# options = webdriver.ChromeOptions()\n",
    "# driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=options)\n",
    "\n",
    "# Chrome 옵션 설정\n",
    "options = Options()\n",
    "options.add_argument('--disable-popup-blocking')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# WebDriver 실행 (ChromeDriver 버전이 Chrome 브라우저 버전과 일치하는지 확인)\n",
    "#driver = webdriver.Chrome(options=options)\n",
    "driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=options)\n",
    "\n",
    "\n",
    "# 네이버 로그인 화면 이동\n",
    "login_url = 'https://nid.naver.com/nidlogin.login'\n",
    "driver.get(login_url)\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "# 아이디& 비밀번호 입력\n",
    "my_id = 'id' # \"id\" 대신에 자신의 네이버 아이디 입력\n",
    "my_pw = 'password' # \"password\" 대신에 자신의 네이버 비밀번호 입력\n",
    "\n",
    "# 로그인 id, pw 입력\n",
    "# 네이버에 로그인 할 경우 'send_keys()' 함수가 아니라 'execute_script()' 함수를 사용\n",
    "driver.execute_script(\"document.getElementsByName('id')[0].value = \\'\" + my_id + \"\\'\")\n",
    "driver.execute_script(\"document.getElementsByName('pw')[0].value = \\'\" + my_pw + \"\\'\")\n",
    "time.sleep(1)\n",
    "\n",
    "# '로그인' 버튼 클릭\n",
    "driver.find_element(By.XPATH, '//*[@id=\"log.login\"]').click()\n",
    "time.sleep(random.uniform(1,1.7))\n",
    "url = 'https://cafe.naver.com/robotclear' # 크롤링할 카페 url 입력\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 수집 함수\n",
    "def extract_date(soup):\n",
    "    try:\n",
    "        date_element = soup.select_one('span.date')\n",
    "        if date_element:\n",
    "            date_text = date_element.text.strip()\n",
    "            # 날짜 형식 처리\n",
    "            if \"시간 전\" in date_text or \"분 전\" in date_text or \"방금\" in date_text:\n",
    "                # 상대적 시간은 현재 날짜로 처리\n",
    "                return time.strftime('%Y-%m-%d', time.localtime())\n",
    "            elif \".\" in date_text:\n",
    "                # YYYY.MM.DD 형식 -> YYYY-MM-DD로 변환\n",
    "                return date_text.replace(\".\", \"-\")\n",
    "        return \"날짜 없음\"  # 기본값\n",
    "    except Exception as e:\n",
    "        logging.error(f\"날짜 추출 오류: {e}\")\n",
    "        return \"날짜 없음\"\n",
    "\n",
    "# 본문 수집 함수\n",
    "def extract_content(soup, include_images=True):\n",
    "    contents = []\n",
    "\n",
    "    # 다양한 본문 선택자 처리\n",
    "    selectors = [\n",
    "        'div.se-module.se-module-text',  # 일반 텍스트 본문\n",
    "        'div.ContentRenderer',           # 특수 서식 본문\n",
    "        'div.scrap_added',               # 스크랩 본문\n",
    "    ]\n",
    "    for selector in selectors:\n",
    "        elements = soup.select(selector)\n",
    "        if elements:\n",
    "            contents.extend([element.text.strip() for element in elements])\n",
    "\n",
    "    # 이미지 포함 여부에 따른 처리\n",
    "    if include_images:\n",
    "        images = soup.select('img')\n",
    "        for img in images:\n",
    "            if img.get('src'):\n",
    "                contents.append(f\"[이미지: {img['src']}]\")  # 이미지 URL 포함\n",
    "\n",
    "    return ' '.join(contents).strip() if contents else \"본문 없음\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 제목, 본문, 댓글, 날짜, 검색 키워드, 게시판 이름 - 빈 list 생성\n",
    "titles = []\n",
    "reviews = []\n",
    "comments = []\n",
    "dates = []\n",
    "search_keywords = []\n",
    "board_titles = []\n",
    "urls = []  # 게시글 URL 저장\n",
    "\n",
    "time.sleep(random.uniform(1, 1.7))\n",
    "url = 'https://cafe.naver.com/robotclear'  # 크롤링할 카페 url 입력\n",
    "driver.get(url)\n",
    "time.sleep(1)\n",
    "\n",
    "time.sleep(1)\n",
    "for k in tqdm(keyword):\n",
    "    print(k, \"키워드 크롤링 중입니다\")\n",
    "\n",
    "    # 검색\n",
    "    search_box = driver.find_element(By.XPATH, '//*[@id=\"topLayerQueryInput\"]')\n",
    "    search_box.send_keys(k)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(1)\n",
    "\n",
    "    driver.switch_to.frame('cafe_main')\n",
    "\n",
    "    # 데이터 수집기간 설정\n",
    "    time_box = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"currentSearchDateTop\"]')))\n",
    "    driver.execute_script(\"arguments[0].click();\", time_box)\n",
    "\n",
    "    # 시작 날짜 입력\n",
    "    start_time = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"input_1_top\"]')))\n",
    "    start_time.click()\n",
    "    time.sleep(1)\n",
    "    start_time.send_keys('20200101')\n",
    "\n",
    "    # 종료 날짜 입력\n",
    "    end_time = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"input_2_top\"]')))\n",
    "    end_time.click()\n",
    "    time.sleep(1.5)\n",
    "    end_time.send_keys('20230912')\n",
    "\n",
    "    # 설정 버튼 클릭\n",
    "    btn_set_top = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"btn_set_top\"]')))\n",
    "    driver.execute_script(\"arguments[0].click();\", btn_set_top)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # 검색 버튼 클릭\n",
    "    search_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"main-area\"]/div[1]/div[1]/form/div[4]/button')))\n",
    "    driver.execute_script(\"arguments[0].click();\", search_button)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    try:\n",
    "        if driver.find_element(By.XPATH, '//*[@id=\"main-area\"]/div[5]/table/tbody/tr/td/div').text.strip() == '등록된 게시글이 없습니다.':\n",
    "            driver.get(url)\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "    except:\n",
    "        print(\"크롤링 시작합미당~\")\n",
    "        pass\n",
    "\n",
    "    # 50개씩 보기\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]').click()\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"listSizeSelectDiv\"]/ul/li[7]/a').click()\n",
    "\n",
    "    # url 가져오기\n",
    "    page_url_list = [a.get_attribute('href') for a in driver.find_elements(By.CSS_SELECTOR, 'a.article')]\n",
    "    first_page = driver.find_element(By.CSS_SELECTOR, 'a.on').get_attribute('href')\n",
    "\n",
    "    # 페이지 넘기기 검색시 최대 100페이지\n",
    "    for j in range(1, 41):\n",
    "        try:\n",
    "            if j % 8 == 0:\n",
    "                # 다음 링크 가져오기\n",
    "                time.sleep(1)\n",
    "                link = first_page[:-1] + str(j)\n",
    "\n",
    "                driver.quit()\n",
    "                driver = webdriver.Chrome()  # 현재 컴퓨터 크롬드라이버 위치로 변경\n",
    "\n",
    "                # 재로그인\n",
    "                login_url = 'https://nid.naver.com/nidlogin.login?mode=form&url=https%3A%2F%2Fwww.naver.com'\n",
    "                driver.get(login_url)\n",
    "                driver.implicitly_wait(10)\n",
    "\n",
    "                driver.execute_script(\"document.getElementsByName('id')[0].value = \\'\" + my_id + \"\\'\")\n",
    "                driver.execute_script(\"document.getElementsByName('pw')[0].value = \\'\" + my_pw + \"\\'\")\n",
    "                time.sleep(1)\n",
    "\n",
    "                # '로그인' 버튼 클릭\n",
    "                driver.find_element('id', 'log.login').click()\n",
    "                time.sleep(1)\n",
    "\n",
    "                # 다음 링크부터 가져오기\n",
    "                driver.get(link)\n",
    "                driver.switch_to.frame('cafe_main')\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if driver.find_element(By.XPATH, '//*[@id=\"main-area\"]/div[5]/table/tbody/tr/td/div').text.strip() == '등록된 게시글이 없습니다.':\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if j > 1:\n",
    "            next_page = first_page[:-1] + str(j)\n",
    "            driver.get(next_page)\n",
    "            time.sleep(1)\n",
    "            driver.switch_to.frame('cafe_main')\n",
    "\n",
    "        page_url_list = [a.get_attribute('href') for a in driver.find_elements(By.CSS_SELECTOR, 'a.article')]\n",
    "\n",
    "        for url in page_url_list:\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(random.uniform(1, 1.7))\n",
    "                driver.switch_to.frame('cafe_main')\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # 게시판 이름 수집\n",
    "                board_title = driver.find_element(By.CSS_SELECTOR, 'div.ArticleTitle').text\n",
    "                board_titles.append(board_title)\n",
    "\n",
    "                # 제목 수집\n",
    "                title = driver.find_element(By.CSS_SELECTOR, 'h3.title_text').text\n",
    "                titles.append(title)\n",
    "\n",
    "                # 날짜 수집 (함수 적용)\n",
    "                soup = bs(driver.page_source, 'lxml')\n",
    "                day = extract_date(soup)\n",
    "                dates.append(day)\n",
    "\n",
    "                # 본문 수집 (함수 적용)\n",
    "                content = extract_content(soup)\n",
    "                reviews.append(content)\n",
    "\n",
    "                # 키워드 수집\n",
    "                search_keywords.append(k)\n",
    "\n",
    "                # URL 수집\n",
    "                urls.append(url)\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # 댓글 수집\n",
    "            try:\n",
    "                iscomment = soup.find_all('span', class_='text_comment')\n",
    "                if len(iscomment) == 0:\n",
    "                    comment = '댓글 없음'\n",
    "                else:\n",
    "                    comment = '\" '.join([c.text.strip() for c in iscomment])  # 댓글을 큰따옴표로 구분\n",
    "                comments.append(comment)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"댓글 수집 중 오류 발생: {e}\")\n",
    "                continue\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "\n",
    "# DataFrame 생성\n",
    "data = {\n",
    "    'date': dates,\n",
    "    'keyword': search_keywords,\n",
    "    'title': titles,\n",
    "    'contents': reviews,\n",
    "    'comments': comments,\n",
    "    'board_titles': board_titles,\n",
    "    'url': urls,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame 생성\n",
    "data = {\n",
    "    'date': dates,\n",
    "    'keyword': search_keywords,\n",
    "    'title': titles,\n",
    "    'contents': reviews,\n",
    "    'comments': comments,\n",
    "    'board_titles': board_titles,\n",
    "    'url': urls,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### url 기준으로 중복 제거 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL 기준 필터링\n",
    "df = df.drop_duplicates(subset='url',keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv 저장 시 이름 꼭 변경해주세요!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"카페명_카테고리명_키워드.csv\", index=False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
