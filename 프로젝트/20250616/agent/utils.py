# agents/utils.py
import torch
from sentence_transformers import SentenceTransformer, models
from qdrant_client import QdrantClient
from openai import OpenAI, AsyncOpenAI # AsyncOpenAI ì„í¬íŠ¸ë„ ì¤‘ìš”í•©ë‹ˆë‹¤!
import os 
from datetime import datetime, date, timedelta
from dateutil.relativedelta import relativedelta
import re
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from qdrant_client.http.models import Filter, FieldCondition, MatchValue 
sentiment_analyzer = None 

# ëª¨ë¸ê³¼ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì €ì¥í•  ì „ì—­ ë³€ìˆ˜
meaning_model = None
topic_model = None
qdrant_client = None
openai_client = None


def get_sentiment_analyzer():
    """
    [ì‹ ê·œ] ì‚¬ì „ í•™ìŠµëœ í•œêµ­ì–´ ê°ì„± ë¶„ë¥˜ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    Hugging Faceì˜ pipelineì„ ì‚¬ìš©í•˜ì—¬ ì‰½ê²Œ êµ¬í˜„í•©ë‹ˆë‹¤.
    """
    global sentiment_analyzer
    if sentiment_analyzer is None:
        print("ğŸŒ€ Loading pre-trained sentiment analysis model...")
        
        # Define the local path to your model files
        local_model_path = "C:/Users/User/DIC_Project/persona_mcp_server/agents/models/bert-nsmc" 
        
        try:
            # Load the tokenizer from your local path
            tokenizer = AutoTokenizer.from_pretrained(local_model_path)
            model = AutoModelForSequenceClassification.from_pretrained(local_model_path)

            # 2. pipelineì— ë¡œì»¬ ëª¨ë¸ê³¼ tokenizer ì „ë‹¬
            sentiment_analyzer = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

        except Exception as e:
            print(f"âŒ Error loading sentiment analysis model from local path: {e}")
            sentiment_analyzer = None # Ensure it remains None if loading fails
    return sentiment_analyzer

def get_embedding_models():
    """
    ì˜ë¯¸ ê²€ìƒ‰(e5-large) ëª¨ë¸ê³¼ ì£¼ì œ ê²€ìƒ‰(ko-sbert) ëª¨ë¸ì„ ëª¨ë‘ ë¡œë“œí•©ë‹ˆë‹¤.
    ì´ë¯¸ ë¡œë“œë˜ì—ˆë‹¤ë©´ ê¸°ì¡´ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    global meaning_model, topic_model
    if meaning_model is None or topic_model is None:
        print("ğŸŒ€ Loading embedding models (meaning & topic)...")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # 1. Meaning Model ë¡œë“œ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        meaning_model = SentenceTransformer(
            modules=[models.Transformer("intfloat/e5-large"), models.Pooling(1024, pooling_mode_mean_tokens=True)],
            device=device
        )
        
        # 2. Topic Model ë¡œë“œ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        topic_model = SentenceTransformer("jhgan/ko-sbert-nli", device=device)
        
        print("âœ… All embedding models loaded.")
        
    return meaning_model, topic_model

def get_qdrant_client():
    """Qdrant í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    global qdrant_client
    if qdrant_client is None:
        print("ğŸŒ€ Initializing Qdrant client...")
        qdrant_client = QdrantClient(host="localhost", port=6333)
        print("âœ… Qdrant client initialized.")
    return qdrant_client

def get_openai_client(async_client=False):
    """
    [ê°œì„ ë¨] OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ìµœì´ˆ í˜¸ì¶œ ì‹œ, í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ì½ì–´ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    global openai_client
    if openai_client is None:
        print("ğŸŒ€ Initializing OpenAI client...")
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("'.env' íŒŒì¼ì— OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        openai_client = OpenAI(api_key=api_key) # í•­ìƒ ë™ê¸° í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì „ì—­ ë³€ìˆ˜ì— ì €ì¥
    return openai_client # ì €ì¥ëœ ë™ê¸° í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°˜í™˜


def get_columns_for_product(product_type: str):
    """Qdrantì—ì„œ íŠ¹ì • ì œí’ˆêµ°ì˜ ìƒì„¸ í•„ë“œ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    print(f"ğŸ”© Getting column info for product_type='{product_type}'...")
    qdrant = get_qdrant_client()
    
    search_filter = Filter(
    must=[
        FieldCondition(
            key="product_type",
            match=MatchValue(value=product_type)
        )
    ]
)
    
    found_points, _ = qdrant.scroll(
        collection_name="product_metadata",
        scroll_filter=search_filter,
        limit=1,
        with_payload=True
    )
    
    if found_points:
        # í˜ì´ë¡œë“œì—ì„œ 'fields' ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        return found_points[0].payload.get("fields", {})
    else:
        # ì¼ì¹˜í•˜ëŠ” ì œí’ˆ ì •ë³´ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        return {}
    
def parse_natural_date(text: str | None) -> tuple[date, date] | None:
    """'ìµœê·¼ 3ê°œì›”', 'ì˜¬í•´' ë“± ìì—°ì–´ ê¸°ê°„ì„ íŒŒì‹±í•˜ì—¬ (ì‹œì‘ì¼, ì¢…ë£Œì¼) íŠœí”Œì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not text:
        return None
    
    today = date.today()
    
    # "ìµœê·¼ Xê°œì›”" í˜•ì‹
    match = re.search(r'ìµœê·¼\s*(\d+)\s*ê°œì›”', text)
    if match:
        months = int(match.group(1))
        start_date = today - relativedelta(months=months)
        return start_date, today

    # "ìµœê·¼ Xì¼" í˜•ì‹
    match = re.search(r'ìµœê·¼\s*(\d+)\s*ì¼', text)
    if match:
        days = int(match.group(1))
        start_date = today - timedelta(days=days)
        return start_date, today
        
    # "ì˜¬í•´"
    if "ì˜¬í•´" in text or "ê¸ˆë…„" in text:
        return date(today.year, 1, 1), today

    # "ì‘ë…„"
    if "ì‘ë…„" in text:
        last_year = today.year - 1
        return date(last_year, 1, 1), date(last_year, 12, 31)
    
    # if "ìµœì‹ " in text or "ìµœê·¼" in text:
    #     # ì •ê·œì‹ìœ¼ë¡œ ê°œì›”/ì¼ì„ ì°¾ì§€ ëª»í•˜ê³  'ìµœì‹ 'ì´ë¼ëŠ” ë‹¨ì–´ë§Œ ìˆìœ¼ë©´ ê¸°ë³¸ê°’(ì˜ˆ: 1ê°œì›”)ìœ¼ë¡œ ì²˜ë¦¬
    #     match_month = re.search(r'(\d+)\s*ê°œì›”', text)
    #     match_day = re.search(r'(\d+)\s*ì¼', text)
    #     if not match_month and not match_day:
    #         print("ğŸŒ€ Interpreting 'ìµœì‹ ' as 'last 1 month'.")
    #         return today - relativedelta(months=1), today
    # --- ì‹ ê·œ ë¡œì§ ë ---

    # "ìµœê·¼ Xê°œì›”" í˜•ì‹
    match = re.search(r'ìµœê·¼\s*(\d+)\s*ê°œì›”', text)
    # TODO: "YYYY-MM-DD ~ YYYY-MM-DD" ë“± ë” ë‹¤ì–‘í•œ í˜•ì‹ ì¶”ê°€ ê°€ëŠ¥
    
    return None # ë§¤ì¹­ë˜ëŠ” í˜•ì‹ì´ ì—†ìŒ