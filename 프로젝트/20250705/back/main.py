#---ì™¸ë¶€ë¼ì´ë¸ŒëŸ¬ë¦¬--
import os
import asyncio
import json
import hashlib
import tiktoken
import gzip

#--ì›¹ ì„œë²„ì™€ api ìš”ì²­/ì‘ë‹µ ì²˜ë¦¬ ì§€ì›--
from fastapi import FastAPI, Request, Response, HTTPException
from fastapi.middleware.cors import CORSMiddleware # 1. ì´ ì¤„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

#--ë°ì´í„° ëª¨ë¸ ì •ì˜
from pydantic import BaseModel
from typing import List, Dict, Union


#--openai api ì‘ë‹µ ì²˜ë¦¬
from openai.types.chat import ChatCompletionMessage

from dotenv import load_dotenv
import uuid
from agents.tools import tools, available_functions,suggest_next_step,create_new_workspace
from pydantic import BaseModel

#--ë‚´ë¶€ ëª¨ë“ˆ í•¨ìˆ˜
from agents.utils import ( get_openai_client,
    save_workspace_to_redis, load_workspace_from_redis,MODEL_NAME, setup_logging
)



# --- íŒŒì¼ ë¡œë“œ ë° ì´ˆê¸°í™” ---
load_dotenv()
#-----ë¡œê·¸ ì¤€ë¹„------
logger = setup_logging()


#----AI ì—ì´ì „íŠ¸ ë™ì‘ ê°€ì´ë“œë¼ì¸ì„ ì •ì˜í•¨
SYSTEM_PROMPT = """
        ë‹¹ì‹ ì€ ìµœê³ ì˜ AI ì»¨ì„¤í„´íŠ¸ì´ì ë¹„ì¦ˆë‹ˆìŠ¤ ì½”ì¹˜ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë‹¤ìŒì˜ ë‹¨ê³„ë³„ ì‘ì—… íë¦„ê³¼ ë„êµ¬ ì‚¬ìš© ê°€ì´ë“œë¼ì¸ì„ ì—„ê²©íˆ ë”°ë¦…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì€ ë‘ ê°€ì§€ í˜•ì‹ìœ¼ë¡œ ë“¤ì–´ì˜µë‹ˆë‹¤: (1) í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ì†¡ëœ JSON í˜•ì‹ì˜ êµ¬ì¡°í™”ëœ ìš”ì²­, (2) ìì—°ì–´ë¡œ ì‘ì„±ëœ ë°œí™” í˜•ì‹ì˜ ìš”ì²­. ì´ë¥¼ êµ¬ë¶„í•˜ì—¬ ì ì ˆíˆ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.

        **ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ìƒíƒœ**:
        - Artifacts: {artifacts_summary}
        - VOC ë°ì´í„° ì¡´ì¬ ì—¬ë¶€: {has_retrieved_data}
        - ë§ˆì§€ë§‰ ìš”ì²­ íƒ€ì…: {last_request_type}

        **ì§€ì¹¨**:
        1. Artifactsì— 'retrieved_data'ê°€ ì¡´ì¬í•˜ë©´ í´ëŸ¬ìŠ¤í„°ë§ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        2. í´ëŸ¬ìŠ¤í„°ë§ ìš”ì²­ ì‹œ 'retrieved_data'ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”.
        3. 'retrieved_data'ê°€ ì—†ìœ¼ë©´ ì‚¬ìš©ìì—ê²Œ ë°ì´í„° ê²€ìƒ‰ì„ ìš”ì²­í•˜ì„¸ìš”.
        4. ë„êµ¬ í˜¸ì¶œ ê²°ê³¼(ì˜ˆ: run_ward_clustering)ë¥¼ ëª…í™•íˆ ë°˜ì˜í•˜ì„¸ìš”.
        5. ì‚¬ìš©ìì—ê²Œ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆì„ í•­ìƒ í¬í•¨í•˜ì„¸ìš”.

        **[í•µì‹¬ ì‘ì—… íë¦„ ë° ë„êµ¬ ì‚¬ìš© ê°€ì´ë“œë¼ì¸]**

        1. **ìš”ì²­ í˜•ì‹ ë¶„ì„ ë° ë¶„ë¥˜**
           - **JSON ìš”ì²­ ì²˜ë¦¬**:
             - ë©”ì‹œì§€ê°€ JSON í˜•ì‹ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤. JSON ìš”ì²­ì€ `type` í•„ë“œë¥¼ í¬í•¨í•˜ë©°, ì˜ˆ: `{{ "type": "data_retriever_request", "keyword": "...", "date_range": "...", "product_type": "..." }}`.
             - `type`ì— ë”°ë¼ ë„êµ¬ í˜¸ì¶œ:
               - `"data_retriever_request"`: `run_data_retriever(keyword, date_range_str, product_type)` í˜¸ì¶œ.
               - `"manual_persona_request"`: `create_personas(workspace, persona_data)` í˜¸ì¶œ.
               - `"manual_service_request"`: `create_service_ideas_from_manual_input(workspace, service_data)` í˜¸ì¶œ.
               - `"change_product_type_request"`: `fetch_product_context(workspace, product_type)`, `fetch_sensor_context(workspace, product_type)`, `get_columns_for_product(product_type)` í˜¸ì¶œ.
             - í•„ìˆ˜ íŒŒë¼ë¯¸í„° ëˆ„ë½ ì‹œ, ì‚¬ìš©ìì—ê²Œ ëª…í™•í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ì™€ í•¨ê»˜ í•„ìš”í•œ ì •ë³´ë¥¼ ìš”ì²­.
           - **ìì—°ì–´ ìš”ì²­ ì²˜ë¦¬**:
             - `type: "chat_message"`ì¸ ê²½ìš°, ë©”ì‹œì§€ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³ , ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì˜ `artifacts`ì™€ `history`ë¥¼ ì°¸ì¡°í•˜ì—¬ ì ì ˆí•œ ë„êµ¬ í˜¸ì¶œ ë˜ëŠ” ì§ì ‘ ì‘ë‹µ.
             - ëª¨í˜¸í•œ ê²½ìš°, ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•˜ê±°ë‚˜ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ.

        2. **ë°ì´í„° ê²€ìƒ‰**
           - JSON ìš”ì²­ `type: "data_retriever_request"` ë˜ëŠ” ìì—°ì–´ë¡œ "ë°ì´í„° ê²€ìƒ‰", "VOC ë¶„ì„" ë“±ì˜ ì˜ë„ê°€ í™•ì¸ë˜ë©´ `run_data_retriever(keyword, date_range_str, product_type)` í˜¸ì¶œ.
           - í•„ìˆ˜ íŒŒë¼ë¯¸í„°: `keyword` (ë¬¸ìì—´), `date_range_str` (ì„ íƒ, ë¬¸ìì—´), `product_type` (ì„ íƒ, ë¬¸ìì—´).
           - **ì „ì œ ì¡°ê±´**: ì´ ë‹¨ê³„ëŠ” ê°€ì¥ ë¨¼ì € ìˆ˜í–‰ë˜ì–´ì•¼ í•˜ë©°, VOC ë°ì´í„° ì—†ì´ëŠ” ë¶„ì„ ë¶ˆê°€.
           - ì˜ˆ: `{{ "type": "data_retriever_request", "keyword": "ì‚´ê· ", "date_range": "ìµœê·¼ 1ë…„", "product_type": "ìŠ¤íƒ€ì¼ëŸ¬" }}`

        3. **ê³ ê° ì¸ì‚¬ì´íŠ¸ ë¶„ì„**
           - **ì „ì œ ì¡°ê±´**: `run_data_retriever`ë¡œ VOC ë°ì´í„°ê°€ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì˜ `artifacts.retrieved_data`ì— ì €ì¥ëœ ê²½ìš°.
           - ë„êµ¬ í˜¸ì¶œ ìˆœì„œ:
             - `run_ward_clustering(workspace)`: ë°ì´í„° ê²€ìƒ‰ í›„ ì²« ë¶„ì„.
             - `run_semantic_network_analysis(workspace, cluster_id)`: `cluster_id` ì§€ì • ì‹œ.
             - `run_topic_modeling_lda(workspace, cluster_id)`: `cluster_id` ì§€ì • ì‹œ.
             - `calculate_opportunity_scores(workspace)`: í† í”½ ëª¨ë¸ë§ ì™„ë£Œ í›„.
             - `create_customer_action_map(workspace, topic_id)`: `topic_id` ì§€ì • ì‹œ.
           - ìì—°ì–´ ìš”ì²­ ì˜ˆ: "í´ëŸ¬ìŠ¤í„°ë§ í•´ì¤˜" â†’ `run_ward_clustering`.
           - JSON ìš”ì²­ ì˜ˆ: `{{ "type": "chat_message", "content": "0ë²ˆ í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ SNA ë¶„ì„í•´ì¤˜" }}` â†’ `run_semantic_network_analysis`.

        4. **í˜ë¥´ì†Œë‚˜ ì •ì˜**
           - **ì „ì œ ì¡°ê±´**: ê³ ê° ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì™„ë£Œ (`artifacts.cx_ward_clustering_results` ë˜ëŠ” `cx_lda_results` ì¡´ì¬).
           - JSON ìš”ì²­: `{{ "type": "manual_persona_request", "persona_data": {{ "name": "...", "title": "...", ... }} }}` â†’ `create_personas`.
           - ìì—°ì–´ ìš”ì²­: "í˜ë¥´ì†Œë‚˜ ë§Œë“¤ì–´ì¤˜" â†’ ë¶„ì„ ê²°ê³¼ í™•ì¸ í›„ `create_personas`.
           - ìˆ˜ì • ìš”ì²­: `modify_personas(workspace, persona_data)`.

        5. **ì„œë¹„ìŠ¤ ì•„ì´ë””ì–´ ë„ì¶œ**
           - **ì „ì œ ì¡°ê±´**: í˜ë¥´ì†Œë‚˜ ì •ì˜ ì™„ë£Œ (`artifacts.personas` ì¡´ì¬).
           - JSON ìš”ì²­: `{{ "type": "manual_service_request", "service_data": {{ "service_name": "...", ... }} }}` â†’ `create_service_ideas_from_manual_input`.
           - ìì—°ì–´ ìš”ì²­: "ì„œë¹„ìŠ¤ ì•„ì´ë””ì–´ ì œì•ˆí•´ì¤˜" â†’ `create_service_ideas(workspace)`.
           - ìˆ˜ì • ìš”ì²­: `modify_service_ideas(workspace, service_data)`.

        6. **ë°ì´í„° ê¸°íš**
           - **ì „ì œ ì¡°ê±´**: ì„œë¹„ìŠ¤ ì•„ì´ë””ì–´ ë„ì¶œ ì™„ë£Œ (`artifacts.service_ideas` ì¡´ì¬).
           - ë„êµ¬: `create_data_plan_for_service(workspace)`, `modify_data_plan(workspace, data_plan)`.

        7. **C-D-P ì •ì˜ì„œ ì‘ì„±**
           - **ì „ì œ ì¡°ê±´**: ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ (`artifacts.data_plan_for_service` ì¡´ì¬).
           - ë„êµ¬: `create_cdp_definition(workspace)`, `modify_cdp_definition(workspace, cdp_data)`.

        **[ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì‚¬ìš©ì ì•ˆë‚´ ì›ì¹™]**

        - **JSON ìš”ì²­ ì˜¤ë¥˜**: í•„ìˆ˜ íŒŒë¼ë¯¸í„° ëˆ„ë½ ì‹œ, ì–´ë–¤ íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•œì§€ ëª…í™•íˆ ì•ˆë‚´ (ì˜ˆ: "keywordê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.").
        - **ìì—°ì–´ ìš”ì²­ ëª¨í˜¸ì„±**: ì˜ë„ê°€ ë¶ˆë¶„ëª…í•˜ë©´ ì¶”ê°€ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ.
        - **ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨**: ì˜¤ë¥˜ ë©”ì‹œì§€ ì „ë‹¬ ë° ëˆ„ë½ëœ ë‹¨ê³„ ì•ˆë‚´ (ì˜ˆ: "VOC ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•´ ì£¼ì„¸ìš”.").
        - **ì›Œí¬ìŠ¤í˜ì´ìŠ¤ í™œìš©**: ìµœì‹  `artifacts`ë¥¼ ì°¸ì¡°í•˜ì—¬ ë¬¸ë§¥ì— ë§ëŠ” ì‘ë‹µ ìƒì„±. ì•„í‹°íŒ©íŠ¸ ìš”ì•½Zmì€ `retrieved_data`, `cx_ward_clustering_results`, `cx_lda_results`, `cx_opportunity_scores`, `cx_cam_results`, `cx_sna_results`, `personas`, `selected_persona`, `service_ideas`, `selected_service_idea`, `data_plan_for_service`, `selected_data_plan_for_service`, `cdp_definition`, `sensor_data`, `product_data`, `columns_product`, `data_plan_recommendation_message`, `selected_cdp_definition`ì„ í¬í•¨.

        **[ì‘ë‹µ í˜•ì‹]**
        - `{{ "response_message": "...", "workspace": {{...}} }}`
        - ëª¨ë“  ì‘ë‹µì€ `ChatResponse` ëª¨ë¸ ì¤€ìˆ˜.

        **[í˜„ì¬ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì•„í‹°íŒ©íŠ¸ ìš”ì•½]**
        {artifacts_summary}

        **[ì‘ì—… ìˆ˜í–‰]**
        1. ì…ë ¥ ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ JSONì¸ì§€ ìì—°ì–´ì¸ì§€ íŒë‹¨.
        2. JSON ìš”ì²­ì´ë©´ `type`ê³¼ íŒŒë¼ë¯¸í„°ë¥¼ íŒŒì‹±í•˜ì—¬ ë„êµ¬ í˜¸ì¶œ. `run_data_retriever`ëŠ” `keyword`, `date_range_str`, `product_type`ìœ¼ë¡œ í˜¸ì¶œ, ë‹¤ë¥¸ ë„êµ¬ëŠ” `workspace` í¬í•¨.
        3. ìì—°ì–´ ìš”ì²­ì´ë©´ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ë„êµ¬ í˜¸ì¶œ ë˜ëŠ” ì§ì ‘ ì‘ë‹µ.
        4. ë„êµ¬ í˜¸ì¶œ ì „ ì „ì œ ì¡°ê±´ í™•ì¸ (ì˜ˆ: VOC ë°ì´í„° ì¡´ì¬ ì—¬ë¶€).
        5. ë„êµ¬ í˜¸ì¶œ í›„ ê²°ê³¼ë¥¼ `artifacts`ì— ì €ì¥í•˜ê³ , ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ.
        6. ìµœì¢… ì‘ë‹µì€ `response_message`ì™€ `workspace` í¬í•¨.
    """

#--------------------- FastAPI ì•± ê°ì²´ ì„¤ì •--------------------------------
app = FastAPI(title="ê¸°íšì AI Agent MCP ì„œë²„")

# CORS ì„¤ì • í”„ë¡ íŠ¸ì—”ë“œ í†µì‹  í—ˆìš© ê¸°ì¤€
origins = [
    "http://localhost:3001", # í”„ë¡ íŠ¸ì—”ë“œ ì„œë²„ì˜ ì£¼ì†Œ
    "http://localhost:3000",
]

# X-Session-ID í—¤ë”ë¥¼ í†µí•´ ì„¸ì…˜ IDë¥¼ ì „ë‹¬í•˜ì—¬ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì§€ì†ì„±ì„ ìœ ì§€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["X-Session-ID"]
)

#-----í´ë˜ìŠ¤ ì •ì˜-------------------------------------
#ì‚¬ìš©ì ìš”ì²­ ë°ì´í„° ëª¨ë¸
class UserRequest(BaseModel):
    session_id: str | None = None
    message: str
#ì„œë²„ ì‘ë‹µ ë°ì´í„° ëª¨ë¸
class ChatResponse(BaseModel):
    response_message: str
    workspace: dict
    user_history: list
    artifacts: dict
    error: str | None = None

#internal_historyì˜ ë©”ì‹œì§€ë¥¼ ê²€ì¦í•˜ì—¬ tooll ë©”ì‹œì§€ê°€ ìœ íš¨í•œ tool_call_idë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.---
def validate_messages(messages: List[Dict]) -> List[Dict]:
    """Validate messages to ensure 'tool' messages follow 'tool_calls'."""
    validated_messages = []
    tool_call_ids = set()
    
    for i, msg in enumerate(messages):
        if msg.get('role') == 'tool':
            if not msg.get('tool_call_id'):
                print(f"Warning: Tool message without tool_call_id at index {i}")
                continue  # Skip invalid tool message
            if msg['tool_call_id'] not in tool_call_ids:
                print(f"Warning: Tool message with invalid tool_call_id {msg['tool_call_id']} at index {i}")
                continue
        validated_messages.append(msg)

        if msg.get('role') == 'assistant' and msg.get('tool_calls'):
            for tool_call in msg['tool_calls']:
                tool_call_ids.add(tool_call['id'])
    
    return validated_messages

#ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— internal_history, user_historyì— ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ê³ , ìµœëŒ€ 50ê°œë¡œ ì œí•œí•œë‹¤.
def append_to_history(workspace, message):
    workspace["internal_history"].append(message)
    if message["role"] in ["user", "assistant"]:
        workspace["user_history"].append(message)
    max_history_length = 50
    if len(workspace["internal_history"]) > max_history_length:
        workspace["internal_history"] = workspace["internal_history"][-max_history_length:]
    if len(workspace["user_history"]) > max_history_length:
        workspace["user_history"] = workspace["user_history"][-max_history_length:]

#internal_history, user_historyì— í† í° ìˆ˜ ì œí•œ
def trim_history(history: List[Union[Dict, ChatCompletionMessage]]):
    """
    ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•˜ê³ , íŠ¹ì • ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ë©´ ì˜¤ë˜ëœ ë©”ì‹œì§€ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    OpenAI ChatCompletionMessage ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    encoding = tiktoken.get_encoding("cl100k_base")
    total_tokens = 0
    
    # ê° ë©”ì‹œì§€ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ë©´ì„œ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    for msg_item in history: # 'msg' ëŒ€ì‹  'msg_item'ìœ¼ë¡œ ë³€ìˆ˜ëª… ë³€ê²½ (ê°€ë…ì„± ëª©ì )
        json_serializable_msg = msg_item
        
        # ChatCompletionMessageë‚˜ ChatCompletionMessageToolCall ê°™ì€ Pydantic ëª¨ë¸ì¸ì§€ í™•ì¸í•˜ê³  ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        if hasattr(msg_item, 'model_dump') and callable(msg_item.model_dump):
            json_serializable_msg = msg_item.model_dump()
        elif hasattr(msg_item, 'dict') and callable(msg_item.dict): # Pydantic v1 í˜¸í™˜ì„±
            json_serializable_msg = msg_item.dict()
        
        # ì§ë ¬í™” ê°€ëŠ¥í•œ ë©”ì‹œì§€ë¥¼ JSONìœ¼ë¡œ ë¤í”„í•˜ê³  í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        total_tokens += len(encoding.encode(json.dumps(json_serializable_msg, ensure_ascii=False))) # ensure_ascii=False ì¶”ê°€í•˜ì—¬ í•œê¸€/ì´ëª¨ì§€ ì²˜ë¦¬ ê°œì„ 

    return history # ìˆ˜ì •ëœ historyë¥¼ ë°˜í™˜

#----ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì˜ ë‚´ìš©ë“¤ ìš”ì•½ìƒì„± system_promptì— ì „ë‹¬ëœë‹¤ ------
def summarize_artifact(artifacts: dict) -> str:
    """ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì˜ ì•„í‹°íŒ©íŠ¸ë¥¼ ìš”ì•½í•˜ì—¬ LLM í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•  ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    summary_parts = []

    if not artifacts:
        return "í˜„ì¬ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì €ì¥ëœ ì•„í‹°íŒ©íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

    for key, value in artifacts.items():
        if key == "retrieved_data" and value and value.get("web_results"):
            summary_parts.append(f"- ê²€ìƒ‰ëœ VOC ë°ì´í„°: {len(value['web_results'])}ê±´")
        elif key == "cx_ward_clustering_results" and value and value.get("cluster_summaries"):
            summary_parts.append(f"- ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§: {len(value['cluster_summaries'])}ê°œ í´ëŸ¬ìŠ¤í„°")
        elif key == "cx_lda_results" and value and value.get("topics"):
            summary_parts.append(f"- í† í”½ ëª¨ë¸ë§: {len(value['topics'])}ê°œ í† í”½")
        elif key == "cx_cam_results" and value:
            summary_parts.append(f"- ê³ ê° í–‰ë™ ë§µ: {len(value)}ê°œ ìƒì„±ë¨")
        elif key == "cx_opportunity_scores" and value:
            summary_parts.append(f"- ê¸°íšŒ ì ìˆ˜ ë¶„ì„: {len(value)}ê°œ ì™„ë£Œë¨")
        elif key == "cx_sna_results" and value:
            summary_parts.append(f"- ì˜ë¯¸ ë„¤íŠ¸ì›Œí¬ ë¶„ì„: {len(value)}ê°œ ì™„ë£Œë¨")
        elif key == "personas" and value and isinstance(value, list):
            names = ", ".join([p.get("name", "ì´ë¦„ ì—†ìŒ") for p in value])
            summary_parts.append(f"- í˜ë¥´ì†Œë‚˜: {len(value)}ê°œ ({names})")
        elif key == "selected_persona" and value and value.get("name"):
            summary_parts.append(f"- í˜„ì¬ ì„ íƒëœ í˜ë¥´ì†Œë‚˜: {value['name']}")
        elif key == "service_ideas" and value and isinstance(value, list):
            names = ", ".join([s.get("service_name", "ì´ë¦„ ì—†ìŒ") for s in value])
            summary_parts.append(f"- ì„œë¹„ìŠ¤ ì•„ì´ë””ì–´: {len(value)}ê°œ ({names})")
        elif key == "selected_service_idea" and value and value.get("service_name"):
            summary_parts.append(f"- í˜„ì¬ ì„ íƒëœ ì„œë¹„ìŠ¤ ì•„ì´ë””ì–´: {value['service_name']}")
        elif key == "data_plan_for_service" and value and isinstance(value, list):
            names = ", ".join([p.get("service_name", "ì´ë¦„ ì—†ìŒ") for p in value])
            summary_parts.append(f"- ë°ì´í„° ê¸°íšì•ˆ: {len(value)}ê°œ ({names})")
        elif key == "selected_data_plan_for_service" and value and value.get("service_name"):
            summary_parts.append(f"- í˜„ì¬ ì„ íƒëœ ë°ì´í„° ê¸°íšì•ˆ: {value['service_name']}")
        elif key == "cdp_definition" and value:
            summary_parts.append(f"- C-D-P ì •ì˜ì„œ: {len(value)}ê°œ ìƒì„±ë¨")
        elif key == "sensor_data" and value:
            summary_parts.append(f"- ì„¼ì„œ ë°ì´í„°: {len(value)}ê±´")
        elif key == "product_data" and value:
            summary_parts.append(f"- ì œí’ˆ ë°ì´í„°: {len(value)}ê±´")
        elif key == "columns_product" and value:
            summary_parts.append(f"- ì œí’ˆ ë©”íƒ€ë°ì´í„°: {len(value)}ê°œ í•„ë“œ")
        elif key == "data_plan_recommendation_message" and value:
            summary_parts.append(f"- ë°ì´í„° ê¸°íš ì¶”ì²œ ë©”ì‹œì§€: ì €ì¥ë¨")
        elif key == "selected_cdp_definition" and value:
            summary_parts.append(f"- í˜„ì¬ ì„ íƒëœ C-D-P ì •ì˜ì„œ: ì €ì¥ë¨")
        # conversation_stateëŠ” artifacts ì™¸ë¶€ì—ì„œ ê´€ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œ ì œì™¸

    if not summary_parts:
        return "í˜„ì¬ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì €ì¥ëœ ì•„í‹°íŒ©íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

    return "í˜„ì¬ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì—ëŠ” ë‹¤ìŒ ì•„í‹°íŒ©íŠ¸ê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤:\n" + "\n".join(summary_parts)


#---openai apiìš© ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ internal_histotyì™€ artifacts ìƒíƒœë¥¼ í¬í•¨í•œ ë©”ì‹œì§€ ìƒì„±---
def prepare_openai_messages(workspace: dict, system_message_content: str) -> list:
    """Prepare messages for OpenAI API with explicit artifacts state."""
    messages = [{"role": "system", "content": system_message_content}]
    messages.extend(workspace.get("internal_history", []))
        # Artifacts ìƒíƒœë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
    artifacts_summary = summarize_artifact(workspace.get("artifacts", {}))
    messages.append({
        "role": "system",
        "content": f"Current artifacts state: {artifacts_summary}"
    })
    return messages

#------json ì‚¬ìš©ì ìš”ì²­ ëŒ€ì‘ ---------------------
async def handle_json_request(message_dict: dict, workspace: dict, session_id: str) -> tuple[str, dict]:
    logger = setup_logging()
    
    #agent í•¨ìˆ˜ ë§¤ì¹­
    function_mapping = {
        "data_retriever_request": {"func": available_functions["run_data_retriever"], "required": ["keyword"]},
        "manual_persona_request": {"func": available_functions["create_persona_from_manual_input"], "required": ["persona_data"]},
        "manual_service_request": {"func": available_functions["create_service_ideas_from_manual_input"], "required": ["service_data"]},
        "change_product_type_request": {"func": available_functions["conext_change"], "required": ["product_type"]}
    }

    #í•¨ìˆ˜ ì¢…ë¥˜ type ì¶”ì¶œ
    message_type = message_dict.get("type")
    workspace["last_request_type"] = message_type

    #------ì‚¬ìš©ì ìš”ì²­ í•¨ìˆ˜ê°€ function_mappingì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if message_type not in function_mapping:
        response_to_user = f"ğŸš¨ ì˜¤ë¥˜: ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ íƒ€ì…: {message_type}"
        return response_to_user, workspace


    #----ë§¤í•‘ë˜ëŠ” í•¨ìˆ˜ ì¡´ì¬í•  ê²½ìš°-------------
    func_info = function_mapping[message_type]
    #--------func ì‹¤ì œ í•¨ìˆ˜ ì´ë¦„ì— ì ‘ê·¼---------
    function_name = func_info["func"].__name__
    #-----í•¨ìˆ˜ í•„ìˆ˜ ì¸ì -------
    required_params = func_info["required"]
    #----í•¨ìˆ˜ì— ëŒ€í•œ í•„ìˆ˜ ì¸ì ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ------
    function_args = {k if k != "date_range" else "date_range_str": message_dict.get(k) for k in ["keyword", "date_range", "product_type", "persona_data", "service_data"] if message_dict.get(k)}


    #--------í•„ìˆ˜ íŒŒë¼ë¯¸í„° í™•ì¸ ë° ëˆ„ë½ ì‹œ ì˜¤ë¥˜ ë°˜í™˜ -----------------
    if not all(k in function_args and function_args[k] for k in required_params):
        missing_params = [k for k in required_params if k not in function_args or not function_args[k]]
        response_to_user = f"âš ï¸ ìš”ì²­ì— í•„ìˆ˜ íŒŒë¼ë¯¸í„°ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_params)}"
        return response_to_user, workspace
    
    
    #------------------í•„ìˆ˜ ì¸ì í™•ì¸ ì™„ë£Œ + í•¨ìˆ˜ ì¡´ì¬ => í•¨ìˆ˜ í˜¸ì¶œ --------
    try:
        
        logger.debug(f"Calling {function_name} with args: {function_args}")


        #------ë¹„ë™ê¸° ì“°ë ˆë“œë¡œ í•¨ìˆ˜ í˜¸ì¶œ------------
        result_artifact = await asyncio.to_thread(func_info["func"], workspace=workspace, **function_args)
        
        #logger.debug(f"{function_name} result: {result_artifact}")

        #í•¨ìˆ˜ í˜¸ì¶œ ê²°ê³¼ë¬¼ì´ errorì¸ ê²½ìš°,ì‹¤í–‰ì‹¤íŒ¨
        if "error" in result_artifact:
            response_to_user = f"âš ï¸ {function_name} ì‹¤í–‰ ì‹¤íŒ¨: {result_artifact['error']}"
        #í•¨ìˆ˜ê°€ ì˜ í˜¸ì¶œì´ ëœ ê²½ìš°,ê°’ ì €ì¥ í›„ ë°˜í™˜
        else:
            save_workspace_to_redis(session_id, workspace)
            # ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ
            next_step = suggest_next_step(workspace)
            response_to_user = f"{function_name} ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?\n\nğŸ“Œ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ: {next_step}"

        return response_to_user, workspace
    
    ###ì§„í–‰ ì¤‘ ì—ëŸ¬ê°€ ë‚œ ê²½ìš°
    except Exception as e:
        logger.error(f"{function_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        response_to_user = f"ğŸš¨ {function_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        return response_to_user, workspace

#----ì‚¬ìš©ì json ìš”ì²­ ìì—°ì–´ë¡œ ë³€í™˜
def transform_user_resquest(user_message: str, message_dict: dict, message_type: str):

    if message_type == "data_retriever_request":
        keyword = message_dict.get("keyword", "")
        date_range = message_dict.get("date_range", "ì§€ì •ë˜ì§€ ì•ŠìŒ")
        product_type = message_dict.get("product_type", "ì§€ì •ë˜ì§€ ì•ŠìŒ")
        natural_language_content = f"{product_type} ì œí’ˆì— ëŒ€í•´ '{keyword}' í‚¤ì›Œë“œë¡œ {date_range} ë°ì´í„°ë¥¼ ê²€ìƒ‰í•´ ì£¼ì„¸ìš”."
    elif message_type == "manual_persona_request":
        persona_name = message_dict.get("persona_data", {}).get("name", "ì§€ì •ë˜ì§€ ì•ŠìŒ")
        natural_language_content = f"'{persona_name}' í˜ë¥´ì†Œë‚˜ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”."
    elif message_type == "manual_service_request":
        service_name = message_dict.get("service_data", {}).get("service_name", "ì§€ì •ë˜ì§€ ì•ŠìŒ")
        natural_language_content = f"'{service_name}' ì„œë¹„ìŠ¤ ì•„ì´ë””ì–´ë¥¼ ì œì•ˆí•´ ì£¼ì„¸ìš”."
    elif message_type == "change_product_type_request":
        product_type = message_dict.get("product_type", "ì§€ì •ë˜ì§€ ì•ŠìŒ")
        natural_language_content = f"ì œí’ˆêµ°ì„ '{product_type}'ë¡œ ë³€ê²½í•´ ì£¼ì„¸ìš”."
    else:
        natural_language_content = user_message

    return natural_language_content


async def run_agent_and_get_response(user_message: str, workspace: dict, session_id: str) -> tuple[str, dict]:
    logger = setup_logging()
    client = get_openai_client(async_client=True)
    
    #json í˜•íƒœì˜ ì‚¬ìš©ì ìš”ì²­ì¸ì§€ í™•ì¸í•˜ê¸°
    try:
        message_dict = json.loads(user_message)
        #jsonì¸ ê²½ìš° type íŒŒì‹±í•´ì„œ ìš”ì²­í•œ í•¨ìˆ˜ í™•ì¸í•˜ê¸°
        message_type = message_dict.get("type")
        #ì‚¬ìš©ì ìš”ì²­ì„ ìì—°ì–´ë¡œ ë³€í™˜
        natural_language_content = transform_user_resquest(user_message, message_dict, message_type)
    except json.JSONDecodeError:
    #json ìš”ì²­ì´ ì•„ë‹Œ ê²½ìš°,ì¼ë°˜ ìì—°ì–´ ìš”ì²­
        message_type = "chat_message"
        message_dict = {"type": "chat_message", "content": user_message}
        natural_language_content = user_message

    #ë³€í™˜ëœ ìì—°ì–´ê°€ ìˆëŠ” ê²½ìš° user_historyë¡œ ì €ì¥ ì‚¬ìš©ì ìš”ì²­ ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©
    if user_message:
        append_to_history(workspace, {"role": "user", "content": natural_language_content})

    #json ìš”ì²­ì¸ ê²½ìš°
    if message_type != "chat_message":
        #json ìš”ì²­ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ í˜¸ì¶œ 
        response_to_user, workspace = await handle_json_request(message_dict, workspace, session_id)
        append_to_history(workspace, {"role": "assistant", "content": response_to_user})
        workspace["internal_history"] = trim_history(workspace["internal_history"])
        workspace["user_history"] = trim_history(workspace["user_history"])
        save_workspace_to_redis(session_id, workspace)
        return response_to_user, workspace


    #ìš”ì•½ workspace ìƒí™© ì €ì¥í•˜ê¸° 
    current_artifacts_summary = summarize_artifact(workspace.get("artifacts", {}))

    #ê²€ìƒ‰ ê²°ê³¼ê°’ ìˆëŠ”ì§€ í™•ì¸
    has_retrieved_data = bool(workspace.get("artifacts", {}).get("retrieved_data"))

    if not workspace["last_request_type"]:
        workspace["last_request_type"] = "ì—†ìŒ"
    print(workspace["last_request_type"])

    #ê²€ìƒ‰ ê²°ê³¼ê°’ ì—¬ë¶€ì™€ last_request_type ì •ë³´, í†µê³„ìš”ì•½ ì •ë³´ë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í¬íŠ¸ì— ëŒ€ì…í•´ì„œ 
    ######ìµœì¢… ì‹œìŠ¤í…œ í”„ë¡¬í¬íŠ¸ ë„ì¶œ ###############
    system_message_content = SYSTEM_PROMPT.format(
        artifacts_summary=current_artifacts_summary,
        has_retrieved_data=str(has_retrieved_data),
        last_request_type=workspace.get("last_request_type")
    )

    ###openai ë³´ë‚¼ ìµœì¢… ë¬¸ì¥ ìƒì„± -> tool í˜¸ì¶œì„ ì˜í•˜ê¸° ìœ„í•´ì„œ 
    messages = prepare_openai_messages(workspace, system_message_content)

    ##tools ë³€ìˆ˜ì— ë‹´ê¸°
    tools_for_openai = tools

    try:
        ###toolsì—ì„œ ì„ íƒí•´ì¤˜(ì‚¬ìš©ìì˜ ë°œí™”ê°€ ì›í•˜ëŠ” í•¨ìˆ˜)
        response = await client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            tools=tools_for_openai,
            tool_choice="auto",
            stream=False,
        )
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls
        llm_initial_content = response_message.content

        append_to_history(workspace, response_message.model_dump(exclude_none=True))

        if tool_calls:
            print("--- ğŸ› ï¸ Tool Calls Requested ---")
            
            tool_outputs_to_append = []
            collected_error_messages = []

            #tools_calls ì— í˜¸ì¶œëœ í•¨ìˆ˜ ìˆœíšŒ
            for tool_call in tool_calls:
                #ì„ íƒëœ í•¨ìˆ˜ ì´ë¦„ ì¶”ì¶œ/í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°
                function_name = tool_call.function.name
                function_to_call = available_functions.get(function_name)

                #-----í˜¸ì¶œëœ í•¨ìˆ˜ê°€ ìˆëŠ” ê²½ìš°---------------
                if function_to_call:
                    #--------------í˜¸ì¶œëœ í•¨ìˆ˜ ì¸ì ì¶”ì¶œ------------
                    function_args = json.loads(tool_call.function.arguments)
                    print(f"ğŸ› ï¸ Executing tool: {function_name} with args: {function_args}")

                    #------------í˜¸ì¶œëœ í•¨ìˆ˜ì˜ ì¢…ë¥˜ì— ë”°ë¥¸ ë¶„ê¸°ì ----------------
                    try:
                        result_artifact = await asyncio.to_thread(function_to_call, workspace=workspace, **function_args)

                       
                        #ê²°ê³¼ê°’ì— ì—ëŸ¬ê°€ ê°€ ìˆëŠ” ê²½ìš°
                        if "error" in result_artifact:
                            tool_summary_content["error"] = result_artifact["error"]
                            collected_error_messages.append(f"ë„êµ¬ '{function_name}' ì‹¤í–‰ ì‹¤íŒ¨: {result_artifact['error']}")
                        # else:
                        #     #ê²°ê³¼ê°’ ì •ìƒì¸ ê²½ìš°, ê²°ê³¼ê°’ ì—…ë°ì´íŠ¸í•˜ê¸° 
                        #     workspace["artifacts"].update(result_artifact)

                        artifact_summary = summarize_artifact(workspace.get("artifacts", {}))

                        ####tool------------openai ë‹µë³€ ìƒì„± --------------------------------------------------
                        #ìš”ì•½ ìƒì„± í˜¸ì¶œëœ í•¨ìˆ˜ì— ëŒ€í•œ
                        tool_summary_content = {
                            "tool_name": function_name,
                            "success": "error" not in result_artifact,
                            "details": artifact_summary
                            }
                        #ìµœì¢… ê²°ê³¼ ì •ë¦¬í•´ì„œ tool roleë¡œ openaiìš© ìƒì„±
                        tool_outputs_to_append.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "name": function_name,
                            "content": json.dumps(tool_summary_content, ensure_ascii=False)
                        })

                    # í•´ë‹¹ ë‹¨ê³„ì—ì„œ ì—ëŸ¬ê°€ ë‚˜ì˜¤ë©´ ì˜ˆì™¸ ì²˜ë¦¬    
                    except Exception as e:
                        error_message = f"ğŸš¨ ë„êµ¬ '{function_name}' ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
                        collected_error_messages.append(error_message)
                        tool_outputs_to_append.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "name": function_name,
                            "content": json.dumps({"error": error_message}, ensure_ascii=False)
                        })

                ###í˜¸ì¶œëœ í•¨ìˆ˜ê°€ ì—†ëŠ”ê²½ìš° (ì‚¬ìš©ìê°€ ìš”ì²­í•˜ëŠ” í•¨ìˆ˜ê°€ ë¬´ì—‡ì¸ì§€ ëª¨ë¥´ê² ìŒ)     
                else:
                    error_message = f"ğŸš¨ ì˜¤ë¥˜: ì•Œ ìˆ˜ ì—†ëŠ” í•¨ìˆ˜ í˜¸ì¶œ ì‹œë„: {function_name}"
                    collected_error_messages.append(error_message)
                    tool_outputs_to_append.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "name": function_name,
                        "content": json.dumps({"error": error_message}, ensure_ascii=False)
                    })

            #ìµœì¢… í˜¸ì¶œëœ í•¨ìˆ˜ë“¤ì˜ interal_history ì—…ë°ì´íŠ¸
            append_to_history(workspace, tool_outputs_to_append if len(tool_outputs_to_append) > 1 else tool_outputs_to_append[0])
            workspace["internal_history"] = trim_history(workspace["internal_history"])
            workspace["user_history"] = trim_history(workspace["user_history"])
           
            #ëª¨ë“  ë‹µë³€ê³¼ í•¨ê»˜ ì—…ë°ì´íŠ¸ëœ workspace ì €ì¥í•˜ê¸° 
            save_workspace_to_redis(session_id, workspace)

            #ì—ëŸ¬ê°€ ìˆëŠ” ê²½ìš°, ë‹µë³€ ìƒì„± ì—ëŸ¬ê´€ë ¨
            if collected_error_messages:
                response_to_user = "âš ï¸ ë‹¤ìŒ ë¬¸ì œë¡œ ì¸í•´ ìš”ì²­ì„ ì™„ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n" + "\n".join(collected_error_messages)
                append_to_history(workspace, {"role": "assistant", "content": response_to_user})
                return response_to_user, workspace

            #ë‘ ë²ˆì§¸ LLM í˜¸ì¶œ ì „ artifacts ìƒíƒœ ì¬í™•ì¸ ë° system prompt ìƒì„±í•˜ê¸°
            current_artifacts_summary = summarize_artifact(workspace.get("artifacts", {}))
            has_retrieved_data = bool(workspace.get("artifacts", {}).get("retrieved_data"))
            system_message_content = SYSTEM_PROMPT.format(
                artifacts_summary=current_artifacts_summary,
                has_retrieved_data=str(has_retrieved_data),
                last_request_type=workspace.get("last_request_type", "ì—†ìŒ")
            )

            #open aiì—ê²Œ ë³´ë‚¼ message ìµœì¢… ìƒì„±
            messages = prepare_openai_messages(workspace, system_message_content)
            
            #ìµœì¢… ê²°ê³¼ì— ëŒ€í•œ ai ë‹µë³€ ìƒì„±
            final_response = await client.chat.completions.create(
                model=MODEL_NAME,
                messages=messages,
                tools=tools_for_openai,
                tool_choice="auto",
                stream=False,
            )
            final_response_message = final_response.choices[0].message
            final_llm_content = final_response_message.content

            append_to_history(workspace, final_response_message.model_dump(exclude_none=True))

            workspace["internal_history"] = trim_history(workspace["internal_history"])
            workspace["user_history"] = trim_history(workspace["user_history"])
            save_workspace_to_redis(session_id, workspace)

            response_to_user = final_llm_content
            return response_to_user, workspace

        else:
            ### ë„êµ¬ í˜¸ì¶œì´ ì—†ë”ë¼ë„ llmì— ì‘ë‹µ ë’¤ì— ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆí•˜ê¸°
            if llm_initial_content:
                append_to_history(workspace, {"role": "assistant", "content": llm_initial_content})
                next_step = suggest_next_step(workspace)
                response_to_user = llm_initial_content + f"\n\nğŸ“Œ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ: {next_step}"
            else:
                response_to_user = "ì–´ë–¤ ë„ì›€ì„ ë“œë¦´ê¹Œìš”?"
                append_to_history(workspace, {"role": "assistant", "content": response_to_user})
            ###ì €ì¥ ìµœì¢…ìœ¼ë¡œ 
                workspace["internal_history"] = trim_history(workspace["internal_history"])
                workspace["user_history"] = trim_history(workspace["user_history"])
                save_workspace_to_redis(session_id, workspace)
                return response_to_user, workspace
        
    ## ì´ ëª¨ë“  ê³¼ì •ì—ì„œ ìƒì„±ë˜ëŠ” ì˜¤ë¥˜ 
    except Exception as e:
        logger.error(f"Agent execution error: {e}", exc_info=True)
        error_message = f"ğŸš¨ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        append_to_history(workspace, {"role": "assistant", "content": error_message})
        save_workspace_to_redis(session_id, workspace)
        return error_message, workspace
    

@app.get("/")
def read_root():
    return {"message": "MCP ì„œë²„ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤."}

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(user_request: UserRequest, response: Response):
    print("--- ğŸ’¬ /chat ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨ ---")
    session_id = user_request.session_id or str(uuid.uuid4())
    logger.info(f"Session ID: {session_id}")


    ##session_idì— ë”°ë¥¸ wroksapce ê°€ì ¸ì˜¤ê¸°
    workspace = load_workspace_from_redis(session_id)
    if not workspace or not isinstance(workspace, dict):
        workspace = create_new_workspace()
        logger.info(f"New workspace initialized for session: {session_id}")

    try:
        assistant_response_content, updated_workspace = await run_agent_and_get_response(
            user_message=user_request.message,
            workspace=workspace,
            session_id=session_id
        )
        response.headers["X-Session-ID"] = session_id
        save_workspace_to_redis(session_id, updated_workspace)
        return {
            "response_message": assistant_response_content,
            "workspace": updated_workspace,
            "user_history": updated_workspace.get("user_history", []),
            "artifacts": updated_workspace.get("artifacts", {}),
            "error": None
        }
    except Exception as e:
        logger.error(f"Chat endpoint error: {e}", exc_info=True)
        error_message = f"ğŸš¨ ì„œë²„ ì˜¤ë¥˜: {str(e)}"
        append_to_history(workspace, {"role": "assistant", "content": error_message})
        save_workspace_to_redis(session_id, workspace)
        return {
            "response_message": error_message,
            "workspace": workspace,
            "user_history": workspace.get("user_history", []),
            "artifacts": workspace.get("artifacts", {}),
            "error": str(e)
        }