import json
from datetime import datetime
from collections import defaultdict

# Qdrant ëª¨ë¸ import
from qdrant_client.http.models import Filter, FieldCondition, MatchValue, Range, SearchRequest, NamedVector

# ê³µìš© ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ import
from .utils import get_embedding_models, get_qdrant_client, get_openai_client


def expand_keywords(keyword: str, product_type: str = None):
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œë¥¼ í™•ì¥í•©ë‹ˆë‹¤.
    1. ìƒí™©/ê²½í—˜ ê¸°ë°˜ì˜ ë¬¸ì¥
    2. ìœ ì‚¬/ì—°ê´€ì–´ ê¸°ë°˜ì˜ ë¬¸ì¥
    ë‘ ì¢…ë¥˜ë¥¼ ëª¨ë‘ ìƒì„±í•˜ë„ë¡ ê³ ë„í™”ë˜ì—ˆìŠµë‹ˆë‹¤.
    """
    client = get_openai_client()
    product_context = f"ğŸ§° ì œí’ˆ ì¹´í…Œê³ ë¦¬: {product_type}\nì´ ë§¥ë½ì„ ë°˜ì˜í•˜ì—¬ ì•„ë˜ ë‚´ìš©ì„ ìƒì„±í•´ì£¼ì„¸ìš”." if product_type else ""
    
    if product_type == None:
        product_context = ""

    # [ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸]
    prompt = f"""
    ë‹¹ì‹ ì€ ì†Œë¹„ì ì–¸ì–´ì™€ ì œí’ˆì˜ ê¸°ìˆ  ìš©ì–´ë¥¼ ëª¨ë‘ ì´í•´í•˜ëŠ” ì†Œë¹„ì ì¸ì‚¬ì´íŠ¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ ì£¼ì–´ì§„ ê¸°ëŠ¥ í‚¤ì›Œë“œì™€ ê´€ë ¨í•˜ì—¬, ë‹¤ìŒ ë‘ ê°€ì§€ ì¢…ë¥˜ì˜ ì†Œë¹„ì í‘œí˜„ì„ í•©ì³ì„œ 10~12ê°œ ìƒì„±í•´ì£¼ì„¸ìš”.

    **ê¸°ëŠ¥ í‚¤ì›Œë“œ: "{keyword}"**
    {product_context}
    ---

    ### 1. ìƒí™©/ê²½í—˜/ë‹ˆì¦ˆë¥¼ í‘œí˜„í•˜ëŠ” ë¬¸ì¥ (5~6ê°œ)
    - ì†Œë¹„ìëŠ” "{keyword}"ë¼ëŠ” ë‹¨ì–´ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    - í•´ë‹¹ ê¸°ëŠ¥ì´ **í•„ìš”í•œ íŠ¹ì • ìƒí™©, ê²ªê³  ìˆëŠ” ë¶ˆí¸í•¨, ë˜ëŠ” ì–»ê³  ì‹¶ì€ ê°€ì¹˜**ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¬¸ì¥ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
    - ì˜ˆì‹œ ('ì‚´ê· ' í‚¤ì›Œë“œ): "ì•„ì´ê°€ ì•„í† í”¼ê°€ ìˆì–´ì„œ ì˜·ì„ ë§¤ë²ˆ ì‚¶ì•„ ì…íˆëŠ”ë° ë„ˆë¬´ ë²ˆê±°ë¡œì›Œìš”."

    ### 2. í‚¤ì›Œë“œë¥¼ ë‹¤ë¥¸ ìš©ì–´ë¡œ í‘œí˜„í•˜ëŠ” ë¬¸ì¥ (4~5ê°œ)
    - ì†Œë¹„ìëŠ” "{keyword}" ëŒ€ì‹ , ê´‘ê³ ë‚˜ ì œí’ˆ ìƒì„¸í˜ì´ì§€ì—ì„œ ë³¸ **ìœ ì‚¬ì–´, ì—°ê´€ ê¸°ìˆ /ë§ˆì¼€íŒ… ìš©ì–´**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§í•˜ê¸°ë„ í•©ë‹ˆë‹¤.
    - ì•„ë˜ ì˜ˆì‹œì²˜ëŸ¼, "{keyword}"ì˜ í•µì‹¬ ê°€ì¹˜ë¥¼ ì „ë‹¬í•˜ëŠ” ë‹¤ë¥¸ í‘œí˜„ì„ ì‚¬ìš©í•œ ë¬¸ì¥ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
    - ì˜ˆì‹œ ('ì‚´ê· ' í‚¤ì›Œë“œ): "ìŠ¤íŒ€ìœ¼ë¡œ 99.9% ì„¸ê· ì„ ë°•ë©¸í•´ì¤€ë‹¤ë‹ˆ ì•ˆì‹¬ë¼ìš”.", "UV ë¨í”„ë¡œ ìœ„ìƒì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆì–´ì„œ ë§ˆìŒì— ë“¤ì–´ìš”."
    
    ---
    **[ê³µí†µ ì œì•½ ì¡°ê±´]**
    - ë‹¨ìˆœ ì¹­ì°¬("ì¢‹ì•„ìš”")ì´ë‚˜ ê°ì • í‘œí˜„ì€ ì§€ì–‘í•´ì£¼ì„¸ìš”.
    - ì‹¤ì œ ì‚¬ìš©ìê°€ ë‚¨ê¸´ í›„ê¸°ë‚˜ ì»¤ë®¤ë‹ˆí‹° ê²Œì‹œê¸€ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬ì—¬ì•¼ í•©ë‹ˆë‹¤.
    - ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ, ê° í•­ëª©ì€ 1ë¬¸ì¥ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
    """
    
    try:
        res = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}], temperature=0.7)
        expanded_list = [line.strip("-â€¢ ") for line in res.choices[0].message.content.split("\n") if line.strip() and "###" not in line]
        
        # --- [í•µì‹¬ ìˆ˜ì •] ---
        # 1. ì›ë³¸ í‚¤ì›Œë“œë¥¼ ë¦¬ìŠ¤íŠ¸ì˜ ë§¨ ì•ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        # 2. setìœ¼ë¡œ ë³€í™˜í–ˆë‹¤ê°€ ë‹¤ì‹œ listë¡œ ë§Œë“¤ì–´ í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.
        final_keywords = [keyword] + expanded_list
        return list(set(final_keywords))
    except Exception as e:
        print(f"í‚¤ì›Œë“œ í™•ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return [keyword]

def summarize_text(text_to_summarize: str):
    """LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤."""
    client = get_openai_client()
    prompt = f"""
    ë‹¹ì‹ ì€ ì†Œë¹„ì ì–¸ì–´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì†Œë¹„ìì˜ ê¸€ ì›ë¬¸ì…ë‹ˆë‹¤.
    ì´ ê¸€ì—ì„œ **ì ì¬ê³ ê°ì˜ ë‹ˆì¦ˆ, ë¶ˆí¸, ìƒí™©, í–‰ë™**ì´ ë“œëŸ¬ë‚˜ëŠ” í•µì‹¬ ë¬¸ì¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ,
    ì›ë¬¸ í‘œí˜„ì„ ìµœëŒ€í•œ ì‚´ë ¤ 3~5ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
    ì›ë¬¸: {text_to_summarize}
    """
    try:
        res = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}], temperature=0.5)
        return res.choices[0].message.content.strip()
    except Exception as e:
        print(f"í…ìŠ¤íŠ¸ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return text_to_summarize

def run_rrf_search(keywords: list, date_range: tuple | None = None, top_k=50, score_threshold=0.5):
    """RRF ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
    meaning_model, topic_model =get_embedding_models()
    qdrant = get_qdrant_client()
    all_hits_map = {}
    rrf_scores = defaultdict(float)
    K_RRF = 60

    # --- [ì‹ ê·œ] ë‚ ì§œ í•„í„° ìƒì„± ë¡œì§ ---
    must_conditions = []
    if date_range and len(date_range) == 2:
        start_date, end_date = date_range
        print(f"ğŸŒ€ Applying date filter: {start_date} ~ {end_date}")
        must_conditions.append(FieldCondition(
            key="date_timestamp", # ğŸ‘ˆ Qdrantì— ì €ì¥ëœ íƒ€ì„ìŠ¤íƒ¬í”„ í•„ë“œëª…
            range=Range(
                gte=int(datetime.combine(start_date, datetime.min.time()).timestamp()),
                lte=int(datetime.combine(end_date, datetime.max.time()).timestamp())
            )
        ))
    query_filter = Filter(must=must_conditions) if must_conditions else None


    for kw in keywords:
        meaning_vec = meaning_model.encode("query: " + kw)
        topic_vec = topic_model.encode(kw)
        search_results = qdrant.search_batch(
            collection_name="web_data",
            requests=[
                SearchRequest(vector=NamedVector(name="meaning", vector=meaning_vec.tolist()), limit=top_k, with_payload=True, filter=query_filter, score_threshold=score_threshold),
                SearchRequest(vector=NamedVector(name="topic", vector=topic_vec.tolist()), limit=top_k, with_payload=True, filter=query_filter, score_threshold=score_threshold)
            ]
        )
        for hits in search_results:
            for rank, hit in enumerate(hits):
                rrf_scores[hit.id] += 1 / (rank + K_RRF)
                if hit.id not in all_hits_map:
                    all_hits_map[hit.id] = hit

    sorted_hit_ids = sorted(rrf_scores.keys(), key=lambda id: rrf_scores[id], reverse=True)
    results = []
    seen_text = set()
    for hit_id in sorted_hit_ids:
        if len(results) >= top_k: break
        hit = all_hits_map[hit_id]
        original_sentence = hit.payload.get("sentence", "")
        if original_sentence and original_sentence not in seen_text:
            result_payload = hit.payload.copy()
            result_payload['id'] = str(hit.id)
            result_payload['original_text'] = original_sentence
            result_payload['score'] = round(rrf_scores[hit.id], 4)
            result_payload['text'] =  summarize_text(original_sentence) if len(original_sentence) > 150 else original_sentence
            results.append(result_payload)
            seen_text.add(original_sentence)
    return results

def fetch_product_context(keyword, product_type=None, top_k=10):
    """Qdrantì—ì„œ ì œí’ˆ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    _, topic_model =  get_embedding_models()
    qdrant =  get_qdrant_client()
    query_filter = None

    print(f"ğŸ” [Debug] fetch_product_context called with product_type: '{product_type}'")

    if product_type and product_type != "(ì„ íƒ ì•ˆí•¨)":
        query_filter = Filter(must=[FieldCondition(key="product_type", match=MatchValue(value=product_type))])
    
    try:
        hits = qdrant.search(
            collection_name="product_data", query_vector=topic_model.encode(keyword).tolist(),
            query_filter=query_filter, limit=top_k, with_payload=True, score_threshold=0.3
        )
        return [hit.payload for hit in hits]
    except Exception as e:
        print(f"ì œí’ˆ ë°ì´í„° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# --- Master Agentê°€ í˜¸ì¶œí•  ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def run_data_retriever(workspace, keyword, product_type, date_range):
    """Data Retriever ì—ì´ì „íŠ¸ì˜ ì „ì²´ ì‘ì—…ì„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜í•©ë‹ˆë‹¤."""
    print(f"âœ… [Agent Called] run_data_retriever: keyword='{keyword}', date_range='{date_range}', 'product_type': '{product_type}'")
    
    # 1. í‚¤ì›Œë“œ í™•ì¥
    expanded_keywords = expand_keywords(keyword, product_type)
    print(f"ğŸ” [Debug] í™•ì¥ëœ í‚¤ì›Œë“œ ëª©ë¡: {expanded_keywords}")
    # 2. ì›¹/ì†Œë¹„ì ë°ì´í„° ê²€ìƒ‰ (RRF)
    web_results = run_rrf_search(expanded_keywords, date_range=date_range)

    # 3. ë‚´ë¶€ ì œí’ˆ ë°ì´í„° ê²€ìƒ‰
    product_results = fetch_product_context(keyword, product_type)
    
    # 4. ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì €ì¥í•  í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ê°€ê³µ
    return {
        "retrieved_data": {
            "query": keyword,
            "web_results": web_results,
            "product_results": product_results,
        },
         "product_type": product_type
    }

def fetch_sensor_context(product_type: str, top_k: int = 10):
    """
    Qdrantì—ì„œ íŠ¹ì • 'Product Category'ì— í•´ë‹¹í•˜ëŠ” ì„¼ì„œ ë°ì´í„° ìƒ˜í”Œì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    qdrant = get_qdrant_client()
    print(f"SENSOR_SEARCH ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ ğŸ” [Debug] Fetching sensor data for Product Category: '{product_type}'")

    if not product_type:
        print("SENSOR_SEARCH_SKIP è£½å“ç¾¤ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚ âš ï¸ product_typeì´ ì œê³µë˜ì§€ ì•Šì•„ ì„¼ì„œ ë°ì´í„° ì¡°íšŒë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    try:
        # 'Product Category' í•„ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•„í„° ìƒì„±
        query_filter = Filter(
            must=[
                FieldCondition(key="Product Category", match=MatchValue(value=product_type))
            ]
        )

        records, _ = qdrant.scroll(
            collection_name="sensor_data",
            scroll_filter=query_filter,
            limit=top_k,
            with_payload=True,
        )
        
        return [record.payload for record in records]
    except Exception as e:
        print(f"SENSOR_SEARCH_ERROR ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ âŒ ì„¼ì„œ ë°ì´í„° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. 'sensor_data' ì»¬ë ‰ì…˜ ë˜ëŠ” 'Product Category' í•„ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return []